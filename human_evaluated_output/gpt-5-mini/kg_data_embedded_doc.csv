Question_ID,Request,Expected_Response,Actual_Response,Voter 1,Voter 2,Voter 3,COMMENTS,Final,,,,,,
IR_1,"Retrieve the SPARQL endpoint status of NoiPA in the date September 7th, 2025?",On,"The SPARQL endpoint status for NoiPA on 2025-09-07 is: ""Available"".

Measurement: http://example.org/measurement/NoiPA/2025-09-07/Sparql%20endpoint  
Endpoint URL (dataset): https://sparql-noipa.mef.gov.it/sparql",1,1,1,,1,,,,,,
IR_1,Is the NoiPA SPARQL endpoint up in the last assessment?,On,"Yes — the latest measurement (prov:generatedAtTime 2025-09-07) reports the SPARQL endpoint as ""Available"".

Measurement: http://example.org/measurement/NoiPA/2025-09-07/Sparql%20endpoint
Endpoint URL: https://sparql-noipa.mef.gov.it/sparql
",1,1,1,,1,,,,,,
IR_2,Which is the amount of data of the LiLa Lemma Bank dataset?,"1,699,687","There are two relevant measurements:

- Amount of data (score): 0.3333333333333333 (measured 2025-09-07) — <http://example.org/measurement/LemmaBank/2025-09-07/Amount%20of%20data%20score>  
- Number of triples (metadata): 1,699,687 (measured 2025-09-07) — <http://example.org/measurement/LemmaBank/2025-09-07/Number%20of%20triples%20%28metadata%29>

Which of these did you need (the normalized score or the raw triple count)?",0.5,1,1,,1,,,,,,
IR_2,How many triples does LiLa Lemma Bank have?,"1,699,687","The LiLa Lemma Bank contains 1,699,687 triples (measurement ""Number of triples (metadata)""). The query-based count (""Number of triples (query)"") is not available.
",1,1,1,,1,,,,,,
IR_3,Which is the license of the DBLP dataset?,CC-0,"The DBLP dataset is released under CC0 (Creative Commons Public Domain Dedication).  

Machine-readable evidence in the measurements:
- http://www.opendefinition.org/licenses/cc-zero  
- https://creativecommons.org/publicdomain/zero/1.0/  

(No human‑readable license string was present in the recorded metadata.)",1,1,1,,1,,,,,,
IR_3,How is DBLP licensed?,CC-0,"The dblp Knowledge Graph is released into the public domain under CC0 1.0.

- Machine-readable metadata: http://www.opendefinition.org/licenses/cc-zero  
- License URL found by querying: https://creativecommons.org/publicdomain/zero/1.0/

Note: a human‑readable license string was not found via the SPARQL endpoint in the collected measurements.",1,1,1,,1,,,,,,
IR_4,Which is the accuracy score of WordNet?,0.00,"The accuracy score for WordNet (w3c-wordnet) is 0.0 — this value appears in the quality measurements (e.g. generated 2025-05-04, 2025-06-01, 2025-07-13, 2025-08-03 and 2025-09-07).
",1,1,1,,1,,,,,,
IR_4,How accurate is WordNet?,0.00,"Short answer: the dataset metadata you provided reports an Accuracy score of 0.0 for the W3C WordNet RDF (measurements on 2025-05-04, 2025-06-01, 2025-07-13, 2025-08-03 and 2025-09-07 all show dqv:value ""0.0"").

What that means (important caveats)
- In this measurement set the Accuracy metric is 0.0 — but that often means the specific “accuracy” test was not applicable or no ground‑truth comparison was carried out, not necessarily that the lexical resource is entirely wrong. Many automated dataset quality pipelines set Accuracy=0 when they cannot compute a meaningful value.  
- Other quality measures in the same metadata (e.g., various F/I/R/A/FAIR components) have non‑zero values, so the overall quality picture is mixed and depends on which property you care about.

Reality about WordNet accuracy (practical view)
- Princeton WordNet is a manually curated lexical database with long usage in NLP and the Semantic Web. It is generally considered high quality for lexical relations (synonymy, hypernymy, antonymy) and lexical coverage for general English, but it is not perfect: sense granularity, missing senses, and occasional annotation mistakes do occur.  
- “Accuracy” depends on the task: for word‑sense disambiguation, semantic similarity, or knowledge‑linking tasks, WordNet’s utility and apparent accuracy vary widely and should be measured against a task‑specific gold standard.

If you want a concrete, useful number
- Tell me which notion of accuracy you mean (e.g., correctness of sense labels, correctness of RDF mappings, performance in a W
",1,1,1,,1,,,,,,
IR_5,What is the trust value of the Linked Open Camera dataset?,0.5,"The trust value for the Linked Open Camera dataset (bpr) is 0.5 — most recently recorded on 2025-09-07.
",1,1,1,,1,,,,,,
IR_5,How trustworthy is the Linked Open Camera dataset?,0.5,"Short answer: moderately trustworthy — usable but only partly endorsed.

Why (latest measurements, 2025-09-07)
- Trust value: 0.5 (medium on a 0–1 scale).
- Accuracy score: 1.0 (very high — the data format/values checked appear correct).
- Reputation score: 7.518489241331637e-06 (extremely low — few or no external reputation signals).
- Availability: explicit download URLs and many RDF/CSV dumps are present (good availability).
- Metadata: ""Author (metadata)"" = False (no author recorded in metadata); publisher info not reported; license metadata not shown in the bpr measurements.
- New vocabulary/terms: new terms/vocabularies are present in the dataset (may need validation).
- Other: no strong signals of security or trusted-provider registration in the available measurements.

Interpretation
- The dataset is technically solid (high accuracy and downloadable RDF/CSV dumps), but its social endorsement and provenance signals are weak (very low reputation, missing author/publisher metadata, no recorded trusted-provider listing). That combination yields a medium trust rating: you can use the data, but you should validate provenance and critical facts before relying on it for high-stakes decisions.

Practical recommendations to increase trust
1. Add clear metadata: author(s), publisher/maintainer, contact email, modification date.
2. Publish a machine-readable license and include it in the metadata.
3. Register the dataset with a trusted provider or repository and obtain a DOI (or other persistent identifier).
4. Provide provenance documentation (how data were produced/curated).
5. Encourage citations / external reuse to build reputation signals.
6. Validate new vocabularies/terms and document their semantics.

If you want, I can:
- extract the exact download URLs and metadata fields found for bpr, or
- produce a checklist or metadata template (DCAT/VoID) you can add to improve trust.",1,1,1,,1,,,,,,
TA_1,Is the number of triples of NoiPA retrieved via the SPARQL endpoint always the same?,"No, they slightly increase over time.","No. The SPARQL (query) counts change over time — they increased in each measurement:

- 2025-05-04: 412,894,535
- 2025-06-01: 419,389,676
- 2025-07-13: 432,861,574
- 2025-08-03: 439,796,613
- 2025-09-07: 446,170,057

(For comparison, the ""number of triples (metadata)"" stays constant at 340,000,000 — the difference reflects that the live dataset exposed by the SPARQL endpoint is being updated.)
",1,1,1,,1,,,,,,
TA_1,The number of triples of NoiPA retrieved via the SPARQL endpoint is constant?,"No, they slightly increase over time.","No — it is not constant. The SPARQL (query) counts for NoiPA changed over time:

- 2025-05-04: 412,894,535
- 2025-06-01: 419,389,676 (+6,495,141)
- 2025-07-13: 432,861,574 (+13,471,898)
- 2025-08-03: 439,796,613 (+6,935,039)
- 2025-09-07: 446,170,057 (+6,373,444)

Overall increase from 2025-05-04 to 2025-09-07: 33,275,522 (~+8.1%).

(For comparison, the metadata triple count recorded in the measurements is constant at 340,000,000.)
",1,1,1,,1,,,,,,
TA_2,Is the SPARQL endpoint of DBLP always available?,Yes,"Short answer: yes — according to the recorded checks it was.

Details:
- The dataset record lists the SPARQL endpoint as https://sparql.dblp.org/sparql.
- Quality measurements for the metric ""Sparql endpoint"" show the value ""Available"" on these check dates: 2025-05-04, 2025-06-01, 2025-07-13, 2025-08-03 and 2025-09-07 (i.e. 5/5 checks).

Note: these are periodic snapshot checks — they indicate the endpoint was up at each measurement time but do not guarantee continuous real‑time availability. If you want, I can (a) compute an uptime percentage from these checks or (b) give a SPARQL/HTTP test you can run to verify availability now.
",1,1,1,,1,,,,,,
TA_2,Is the DBLP SPARQL endpoint always up?,Yes,"Short answer: According to the recorded quality measurements, yes — the DBLP SPARQL endpoint was reported as ""Available"" on all checks.

Recorded checks and values:
- 2025-05-04 — Available
- 2025-06-01 — Available
- 2025-07-13 — Available
- 2025-08-03 — Available
- 2025-09-07 — Available

Endpoint URL (from the metadata): https://sparql.dblp.org/sparql

Caveat: those are periodic checks — they show the endpoint was up at each measurement time but cannot guarantee it is up at every moment between/after those checks. If you want to verify right now or set continuous monitoring you can run a quick test, for example:

curl -G --data-urlencode 'query=ASK{}' 'https://sparql.dblp.org/sparql' -H 'Accept: application/sparql-results+json'

A successful response (HTTP 200 with a JSON result) indicates the endpoint is responding. I can also give a script or monitoring suggestions if you want.
",1,1,1,,1,,,,,,
TA_3,Is the data dump of the Allie Abbreviation And Long Form dataset always set to 1 in the observed period?,"No, it is discontinuous","No. The ""data dump"" metric (Availability of RDF dump / Availability for download (metadata)) was 1 from 2025-05-04 through 2025-08-03, but it changed to 0 on 2025-09-07.

Measurements:
- 2025-05-04: 1
- 2025-06-01: 1
- 2025-07-13: 1
- 2025-08-03: 1
- 2025-09-07: 0
",1,1,1,,1,,,,,,
TA_3,Is the data dump of the Allie Abbreviation And Long Form dataset always up?,"No, it is discontinuous","Short answer: No — the dump is not guaranteed to be always up.

Evidence from the dataset quality measurements:
- Historical checks (2025-05–2025-08) show Availability of RDF dump (metadata) = 1 (a dump was advertised), but Availability of RDF dump (query) = False (the automatic download/query check failed).
- The most recent measurement (2025-09-07) reports Availability of RDF dump (metadata) = 0 and an overall Availability score = 0.0, indicating the dump was not available at that time.

What to do: visit the dataset page (http://allie.dbcls.jp/) or contact the publisher (Database Center for Life Science — email shown in the metadata: info AT dbcls.rois.ac.jp) to confirm current dump availability.
",1,1,1,,1,,,,,,
TA_4,Is the accuracy of NoiPA constant over time?,No,"No — it changed slightly. Measurements for NoiPA’s ""Accuracy score"":

- 2025-05-04: 0.99970  
- 2025-06-01: 0.99970  
- 2025-07-13: 0.99966  
- 2025-08-03: 0.99966  
- 2025-09-07: 0.99966

Absolute change = −0.00004 (≈ −4×10⁻⁵), ≈ −0.004% relative — a very small decrease from June to July that then remained stable.
",1,1,1,,1,,,,,,
TA_4,Does the accuracy of NoiPA remain consistent over time?,No,"Short answer: yes — the accuracy is essentially constant and very high.

Details:
- 2025-05-04: 0.99970  
- 2025-06-01: 0.99970  
- 2025-07-13: 0.99966  
- 2025-08-03: 0.99966  
- 2025-09-07: 0.99966

There is a very small drop from 0.99970 to 0.99966 (absolute change 0.00004, ≈0.004% relative), after which the value stays stable. If you want, I can plot the trend or compute summary statistics (mean, std. dev., relative change).
",1,1,1,,1,,,,,,
TA_5,What is the longest continuous period in which the Allie Abbreviation And Long Form dataset data dump has been equals to 1?,91 days,"Assuming you mean the ""Availability of RDF dump (metadata)"" measurement: the longest continuous period with value = 1 runs from 2025-05-04 through 2025-08-03 (inclusive) — a total of 92 days.
",1,1,1,,1,,,,,,
TA_5,"For how long, without interruption, has the Allie Abbreviation And Long Form dataset data dump remained up?",91 days,"Based on the measurements, the RDF dump was recorded as available on 2025-05-04, 2025-06-01, 2025-07-13 and 2025-08-03, and then marked unavailable on 2025-09-07. Therefore it was continuously up for at least 92 days (from 2025-05-04 through 2025-08-03). Note: checks are monthly, so the exact time of the later outage is only known to fall between 2025-08-03 and 2025-09-07.
",1,1,1,,1,,,,,,
Co_1,Which dataset has the highest number of triples?,DBLP,"The dataset dblp-kg has the most triples — 1,452,862,425 triples (measurement: Number of triples (query), prov:generatedAtTime 2025-09-07).
",1,1,1,,1,,,,,,
Co_1,Which is the biggest dataset among the compared ones?,DBLP,"The dblp-kg dataset. Latest measurements show ~1,452,862,425 triples (measurement 2025-09-07). 

For comparison (latest values in the data):
- dblp-kg: 1,452,862,425 triples
- NoiPA: 446,170,057 triples
- bpr: 354,148,161 triples
- LemmaBank: 1,699,687 triples (metadata)
- w3c-wordnet: 710,000 triples (metadata)",1,1,1,,1,,,,,,
Co_2,Which is the dataset performing worst in the license metric?,CIDOC-CRM,"CIDOC-CRM — Licensing score = 0.0 (measurements on 2025-05-04, 2025-06-01, 2025-07-13, 2025-08-03 and 2025-09-07). This indicates no license was detected.
",1,1,1,,1,,,,,,
Co_2,Which is the worst performing dataset in the license metric?,CIDOC-CRM,"CIDOC-CRM — licensing score 0.0 (no license retrievable / indicated).
",1,1,1,,1,,,,,,
Co_3,What is the mean score for the accuracy dimension?,0.50,"Using all Accuracy score measurements in the provided data (39 measurements total), the mean accuracy is:

mean = 18.1983784 / 39 ≈ 0.46662509

Rounded: 0.46663.

(Computation used all /Accuracy score measurements across datasets and dates: LemmaBank 5×0.0, NoiPA 5 values (two 0.9997, three 0.99966), w3c-wordnet 5×0.0, bpr 5 values (0.6,0.6,1.0,1.0,1.0), dblp-kg 5×1.0, CIDOC-CRM 5×0.0, bbc-programmes 5×0.0, allie-abbreviation 4×0.9999996.)
",0,0,0,,0,,,,,,
Co_3,What is the mean value of the accuracy dimension?,0.50,"Mean accuracy ≈ 0.46662509.

(Computed as the average of 39 Accuracy score measurements: sum = 18.1983784, count = 39 → 18.1983784 / 39 ≈ 0.46662509.)
",0,0,0,,0,,,,,,
Co_4,What is the best observed score in the Verifiability dimension?,"0.4983 (NoiPA, Allie, Bathing Water Quality)","The best observed Verifiability score is 0.4983333333333333 (e.g. for the dataset NoiPA).
",0.5,0.5,0.5,,"0.5",,,,,,
Co_4,What is the highest observed Verifiability score?,"0.4983 (NoiPA, Allie, Bathing Water Quality)","The highest observed Verifiability score is 0.4983333333333333 (≈ 0.4983).  
This value appears, for example, for the NoiPA dataset (and for environment-agency-bathing-water-quality).
",0.5,0.5,0.5,,"0.5",,,,,,
Co_5,What are the highest-performing dimensions in September 2025?,"License, Performance, Accuracy, Believability, Currency, Timeliness, Security, Conciseness and Interpretability","I can do that — could you clarify what you mean by “highest-performing dimensions”?

Options I can provide (pick one or more):
- Per-dataset on 2025-09-07: list each dataset and its top-scoring quality dimensions (and their scores) for measurements generated 2025-09-07.
- Global (all datasets) on 2025-09-07: compute an aggregate (mean or median) score per dimension across datasets and return the top N dimensions.
- Show top N individual metric measurements (metric → value → dataset) from 2025-09-07 and map them to their dimensions.
- Something else — please specify.

Which do you want? If global aggregation, tell me whether to use mean or median and how many top dimensions to return (default: top 5, mean).
",0,0,0,,0,,,,,,
Co_5,What are the dimensions performing better in September 2025?,"License, Performance, Accuracy, Believability, Currency, Timeliness, Security, Conciseness and Interpretability","Looking at the measurements with prov:generatedAtTime ""2025-09-07"" (the snapshot for Sept 7, 2025), the dimensions that consistently show the highest scores are:

- Accessibility (A score)
  - Examples: bpr A score = 1.0; NoiPA A score = 1.0; LemmaBank A = 0.75.
  - Interpretation: several datasets reach the top value (or near it) for accessibility.

- Security
  - Examples: bpr Security score = 1.0; NoiPA Security score = 0.5; dblp-kg Security = 0.5.
  - Interpretation: at least some datasets (notably bpr) have maximal security scores.

- Accuracy
  - Examples: bpr Accuracy score = 1.0; NoiPA Accuracy score = 0.99966.
  - Interpretation: accuracy is high for the best-performing datasets in this snapshot.

- Performance
  - Examples: NoiPA Performance score = 1.0; dblp-kg Performance score = 1.0; bpr Performance ≈ 0.5075.
  - Interpretation: some datasets attain top performance values (others are moderate).

- Reusability (R score)
",0,0,0,,0,,,,,,
E&D_1,Why is the availability score equals to 1 for the DBLP dataset?,Because all the metrics in the availability dimension can be correctly computed,"The availability score is 1.0 because all of the availability-related checks used to compute that metric are satisfied for DBLP (presence of a dataset page/URL, downloadable RDF metadata/dumps, working access points and a reachable SPARQL endpoint, no auth required). Evidence in the measurements you provided:

- Dataset URL present
  - /dblp-kg/2025-08-03/Dataset URL → ""https://dblp.org""
  - /dblp-kg/2025-06-01/Dataset URL → ""https://dblp.org""

- Download URLs / RDF dump (metadata) available
  - /dblp-kg/2025-08-03/URL for download the dataset → ""['https://doi.org/10.4230/dblp.rdf.ntriples', 'https://dblp.org/rdf/dblp.ttl.gz']""
  - /dblp-kg/2025-08-03/Availability for download (metadata) → value ""1"" (and same for 2025-05-04, 2025-06-01, 2025-07-13)

- Working access points and endpoints
  - /dblp-kg/2025-08-03/A1-D Working access point(s) → ""1.0""
  - /dblp-kg/2025-08-03/Sparql endpoint → ""Available""
  - /dblp-kg/2025-08-03/Requires authentication → ""False"" (no auth barrier)

- Metadata accessibility / discovery
  - /dblp-kg/2025-08-03/A1-M Metadata availability via working primary sources → ""1""
  - /dblp-kg/2025-08-03/A2-M Registered in search engines → ""1""

Because those key availability criteria are present (and typically given full weight in the availability sub-score), the computed Availability score is 1.0 on the measurement dates shown (e.g. 2025-05-04, 2025-06-01, 2025-07-13, 2025-08-03).

If you meant the 2025-09-07 measurement that shows 0.75 at the top of your dump: that indicates some availability-related component changed or was re-weighted for that run — tell me if you want I can compare the 2025-09-07 sub-metrics to earlier runs and point out which component caused the drop.
",0,0,0.5,,0,,,,,,
E&D_1,Why does the DBLP dataset have an availability score of 1?,Because all the metrics in the availability dimension can be correctly computed,"Short answer: because DBLP exposes working access points and metadata without authentication (SPARQL endpoint, downloadable metadata, dataset URL, machine-readable license, HTTPS support), so the availability metric's sub-checks are all satisfied at the time the score 1.0 was produced.

Evidence from the measurements you gave (examples with timestamps):

- A1-D Working access point(s): 1.0 (2025-07-13, 2025-08-03, 2025-06-01)
- A1.2 Authentication & HTTPS support: 1.0 (2025-06-01, 2025-07-13, 2025-08-03)
- Requires authentication: False (2025-05-04, 2025-06-01, 2025-07-13, 2025-08-03)
- Sparql endpoint: ""Available"" (2025-05-04, 2025-06-01, 2025-07-13, 2025-08-03)
- Availability for download (metadata): ""1"" (2025-05-04, 2025-06-01, 2025-07-13, 2025-08-03)
- Availability of RDF dump (metadata): ""1"" (2025-05-04, 2025-06-01, 2025-07-13, 2025-08-03)
- Dataset URL: ""https://dblp.org"" (present in measurements on 2025-05-04, 2025-06-01, 2025-07-13, 2025-08-03)
- License machine-readable (query): ""https://creativecommons.org/publicdomain/zero/1.0/"" (2025-05-04, 2025-06-01, 2025-07-13, 2025-08-03)
- Availability of a common accepted media type: True (2025-05-04, 2025-06-01, 2025-07-13, 2025-08-03)

Because the availability score is computed from these sub-metrics (presence of working access points, downloadability of metadata, reachable endpoints, no authentication barriers, machine-readable license, accepted media types, etc.), and DBLP satisfied them at the 2025-07-13 / 2025-08-03 checks, the overall availability score was 1.0.

Note: the same dataset has different availability values at other dates in your data (e.g. 0.5 on 2025-06-01 and 0.75 on 2025-09-07), which indicates the check results or weighting changed over time (or some sub-checks failed/were interpreted differently). If you want, I can (a) show exactly which sub-metrics changed between dates, or (b) compute a breakdown showing how the metric formula maps sub-checks to the final availability score. Which would you prefer?
",0,0,0.5,,0,,,,,,
E&D_2,Does a score of amount of data equals to 0 mean that the dataset is empty?,"No, it means that the amount of data cannot be computed via the SPARQL endpoint and it is not declared in the metadata.","Short answer: No — a 0 ""amount of data"" score does not automatically mean the dataset is empty. It means the dataset scored the minimum on that particular metric scale (usually [0,1]) for the chosen measurement method. Whether that corresponds to an actually empty graph depends on how the metric is defined and on the underlying counts used to compute it.

Notes and how to check:

- What 0 can indicate
  - The dataset has (by the metric) effectively no data for the measured dimension (e.g., no labelled resources, no entities, etc.).
  - The metric threshold/normalization produced a 0 (e.g., very small counts relative to the expected range).
  - The measurement failed or the metric is not applicable — sometimes missing/unsupported measurements are recorded as ""-"" or not present rather than 0, but you should check the metric definition and provenance.

- How to be sure whether the dataset is actually empty
  1. Check raw counts used by the metric: look for measurements such as ""Number of triples"", ""Number of entities"", ""Number of labels/comments"", ""Number of blank nodes"". If any of those counts are > 0 the dataset is not empty.
  2. Check whether an explicit ""Number of triples"" or similar measurement exists for that dataset and time (and its dqv:value).
  3. Inspect the dataset (download or SPARQL endpoint) directly: a COUNT(*) of triples is definitive.

- Example SPARQL queries you can run against the measurement graph to verify counts (adapt the dataset IRI):
  - Get latest amount-of-data score for a dataset:
    SELECT ?m ?value ?time WHERE {
      ?m a <http://www.w3.org/ns/dqv#QualityMeasurement> ;
         <http://www.w3.org/ns/dqv#computedOn> <http://example.org/dataset/dblp-kg> ;
         <http://www.w3.org/ns/dqv#isMeasurementOf> ?metric ;
         <http://www.w3.org/ns/dqv#value> ?value ;
         <http://www.w3.org/ns/prov#generatedAtTime> ?time .
      FILTER(CONTAINS(str(?metric), ""Amount%20of%20data"") || CONTAINS(str(?metric), ""Amount of data""))
    }
    ORDER BY DESC(?time)
    LIMIT 1
  - Find numeric counts for the dataset:
    SELECT ?metric ?value ?time WHERE {
      ?m a <http://www.w3.org/ns/dqv#QualityMeasurement> ;
         <http://www.w3.org/ns/dqv#computedOn> <http://example.org/dataset/dblp-kg> ;
         <http://www.w3.org/ns/dqv#isMeasurementOf> ?metric ;
         <http://www.w3.org/ns/dqv#value> ?value ;
         <http://www.w3.org/ns/prov#generatedAtTime> ?time .
      FILTER(CONTAINS(lcase(str(?metric)), ""number%20of%20triples"") ||
             CONTAINS(lcase(str(?metric)), ""number%20of%20entities"") ||
             CONTAINS(lcase(str(?metric)), ""number%20of%20labels"") ||
             CONTAINS(lcase(str(?metric)), ""number%20of%20blank%20nodes""))
    }
    ORDER BY DESC(?time)
  - If you have SPARQL access to the dataset endpoint, count triples directly:
    SELECT (COUNT(*) AS ?triples) WHERE { ?s ?p ?o }

- Practical example from your data
  - Your measurement payload uses a scaled ""Amount of data score"" in [0,1] (see the metric documentation included). A 0 would mean worst/lowest on that scale; but datasets in your listing typically have 0.333... or 0.666..., not 0. To determine emptiness, look for ""Number of triples"" or run a triple count as shown above.

If you want, tell me which dataset IRI you care about and I can (a) point to the relevant measurements in your listing, or (b) produce the exact SPARQL query to run against your measurement store or the dataset endpoint to confirm whether it contains any triples.
",0.5,0.5,0.5,,"0.5",,,,,,
E&D_2,"If the amount-of-data score is 0, does that mean the dataset contains no data?","No, it means that the amount of data cannot be computed via the SPARQL endpoint and it is not declared in the metadata.","Short answer: No — a 0 value for an ""amount-of-data"" metric does not necessarily mean the dataset literally contains zero triples. It usually means the dataset scored zero on that particular normalized metric for the way it was computed (or the metric could not be measured), not that the dataset has no data at all.

Why not:
- Many ""amount of data"" metrics are normalized or thresholded (range [0,1]). A 0 can mean ""extremely small"" relative to the reference scale, or below the threshold used for scoring.
- A 0 can also result from missing inputs for the metric (e.g. no accessible SPARQL endpoint or RDF dump). In your RDF measurements some metrics use ""-"" to indicate the endpoint was missing and ""False"" to indicate a negative boolean result.
- Measurement errors or the metric measuring a subset (e.g. only metadata triples) can produce 0 even if the dataset has other kinds of triples.

Illustrative examples from your data:
- dblp-kg has ""Number of triples (metadata)"" = 0 on several dates but its ""Amount of data score"" = 0.333… — that shows zero metadata triples does not mean the dataset has no data at all.
- Many measurements show ""-"" for counts such as ""Number of entities"" when the endpoint or query was not available; that does not imply the dataset contains zero entities — it means the measurement could not be produced.

How you can check whether a dataset actually contains no data:
1. If there's a public SPARQL endpoint: run a simple triple count:
   SELECT (COUNT(*) AS ?triples) WHERE { ?s ?p ?o }
2. If you can download an RDF dump: count non-comment lines or parse and count triples (e.g. use RDF tools like rapper/riot or run wc -l on N-Triples).
3. Inspect other measurements in your metadata: ""Number of entities"", ""Number of labels/comments"", ""Number of blank nodes"", ""Number of triples (query)"" — if these are non-zero, there is data.
4. Look for values ""-"" or ""[]"": they usually indicate the measurement could not be performed (missing endpoint, missing dump), not that the count is zero.

If you want, tell me which dataset URI from your list (for example <http://example.org/dataset/dblp-kg> or <http://example.org/dataset/LemmaBank>) and which measurement date you care about and I can point to the relevant measurements in your file or give the exact SPARQL query/command you should run to confirm the actual triple count.
",0.5,0.5,0.5,,"0.5",,,,,,
E&D_3,"When the availability score is 1, does it mean that both the SPARQL endpoint and the data dump are working?",Yes,"Short answer: Yes. In this catalogue a value of 1 for the ""Availability score"" indicates full availability — i.e. the dataset has an operational SPARQL endpoint and an RDF dump available for download.

Evidence in the data:
- dblp-kg has Availability score = 1.0 and its Sparql endpoint = ""Available"" and Availability of RDF dump (metadata) = 1.
- w3c-wordnet has Availability score = 0.5 while its Sparql endpoint is missing (""-"") but the dump metadata = 1 — showing 0.5 corresponds to only one of the two access mechanisms being available.

(There is a wording inconsistency in the A1-D metric text in the RDF, but the measurements above make clear that an Availability score of 1 means both access points are working. If you want, I can extract the exact metric definition or list which datasets currently have score = 1.)
",1,1,1,,1,,,,,,
E&D_3,"if the availability score is 1, does that mean both the SPARQL endpoint and the data dump are up?",Yes,"Short answer: No — an availability score of 1 does not necessarily mean both the SPARQL endpoint and an RDF dump are up. It means there is an operational access point (SPARQL OR an accessible dump) according to the metric definitions used.

Why:
- The A1-D ""Working access point(s)"" metric (shown in your data) defines: ""1: operational SPARQL or accessible data dump; 0.5: accessible SPARQL endpoint or data dump; 0: otherwise."" So a 1 reflects that at least one access point is operational, not necessarily both.
- Your data contains an explicit example that illustrates this:  
  - <http://example.org/measurement/dblp-kg/2025-08-03/Availability%20score> value ""1.0""  
  - At the same date <http://example.org/measurement/dblp-kg/2025-08-03/Sparql%20endpoint> value ""Available""  
  - But <http://example.org/measurement/dblp-kg/2025-08-03/Availability%20for%20download%20%28query%29> value ""False""  
  That shows Availability score = 1 while the RDF dump check (query) is False — i.e. SPARQL is up but the dump check failed.

How to tell if both are up:
- Inspect the per-dataset measurements for the specific metrics:
  - Sparql endpoint (e.g. .../Sparql%20endpoint) — should be ""Available"" (or equivalent).
  - Availability for download (query) and/or Availability for download (metadata) or URL for download the dataset — should indicate the dump is present and downloadable (e.g. ""True""/1 or a non-empty URL).
- If you need a combined metric that requires both, compute it yourself by requiring both submetrics to be positive (e.g., both Sparql endpoint = Available AND Availability-for-download query/metadata = True).

If you want, I can extract and list for a given dataset and date whether each of those specific measurements is true/available. Which dataset/date should I check?
",0,0,0,,0,,,,,,
E&D_4,"if license is set to 1, does it mean that an open license is used?","No, it means that a license is specified","Short answer: No — a value of 1 for the R1.1 metric only means ""a license was explicitly reported / retrievable"", it does not by itself indicate that the license is open.

Why: the R1.1 metric (labelled ""Machine- or human-readable license retrievable via any primary source"") has ex:MetricOutput ""1: license explicitly reported; 0: otherwise"". That is a presence check. Openness is a separate property of the license itself and must be determined by inspecting the reported license identifier/URI.

How to tell if the reported license is open
- Look up the machine-readable license measurement (the metrics named like ""License machine redeable (metadata)"" or ""License machine redeable (query)""). Their dqv:value holds the license URI/string (for example ""http://www.opendefinition.org/licenses/cc-zero"", ""http://www.opendefinition.org/licenses/cc-by-sa"", ""http://creativecommons.org/licenses/by/4.0/"", etc).
- Compare that URI to a known list of open licenses (e.g., SPDX identifiers/URLs, Open Definition canonical license URIs, known Creative Commons public-domain / permissive licenses such as CC0, CC-BY, ODbL, OGL, OGL-UK, OGL-UK-3.0, OGL‑UK‑3.0‑AS, OGL-3.0, OGL‑3.0, ODC-BY, etc).
- If the license URI matches a known open license, then the dataset is under an open license; otherwise it may be restrictive or unknown.

Example SPARQL (conceptual) to get the reported license for a dataset:
SELECT ?m ?value ?time WHERE {
  ?m a dqv:QualityMeasurement ;
     dqv:computedOn <http://example.org/dataset/LemmaBank> ;
     dqv:isMeasurementOf <http://example.org/Quality-Metric/License%20machine%20redeable%20%28metadata%29> ;
     dqv:value ?value ;
     prov:generatedAtTime ?time .
}
ORDER BY DESC(?time)

If you want, I can:
- run/produce a SPARQL query for a particular dataset from your RDF to fetch the license URIs,
- provide a small mapping table of common license URIs → open / not-open,
- or give code to automatically classify license URIs as open using SPDX/Open Definition lists.
",0.5,0.5,1,,"0.5",,,,,,
E&D_4,"If license is set to 1, does it mean that an open license is used?","No, it means that a license is specified","Short answer: No — a value of ""1"" for the R1.1 metric only means a license was explicitly reported/retrievable; it does not imply the license is open.

Why:
- The metric R1.1 is defined (in your data) with ex:MetricOutput ""1: license explicitly reported; 0: otherwise"". So 1 = license present/retrievable, not a statement about its openness or permissions.
- To know whether the license is ""open"" you must inspect the actual license identifier/URI reported (the other metrics in your data — e.g. License machine readable (metadata) or (query) — return the license URI or string).

Examples from your measurements:
- dblp-kg (query/metadata) -> ""https://creativecommons.org/publicdomain/zero/1.0/"" (CC0) — open.
- micro-coronavirus -> ""http://www.opendefinition.org/licenses/cc-by"" / ""http://creativecommons.org/licenses/by/4.0/"" (CC BY) — open.
- bpr -> ""https://creativecommons.org/licenses/by/3.0/deed.it"" (CC BY 3.0 Italy) — open.
- CIDOC-CRM -> R1.1 = ""0"" (no license reported).

How to determine openness reliably:
- Take the license URI/value reported by the machine-readable metric and:
  - Check it against a curated list of open licenses (Open Definition, SPDX, Creative Commons, ODbL, OGL, public domain, etc.).
  - Or resolve the URI and inspect the license text/label to decide whether it satisfies your openness criteria.
- For automation, map reported license URIs to SPDX identifiers or to an ""isOpen"" list and mark datasets accordingly.

If you want, I can:
- Produce a small SPARQL/pseudocode example that selects datasets with R1.1 = 1 and lists their reported license URIs, or
- Provide a recommended curated list of license URIs you can use to classify licenses as open/non-open. Which would you prefer?
",0.5,0.5,1,,"0.5",,,,,,
E&D_5,"If verifiability is set to 1, does it mean that all information about providers are set?",Yes,"Short answer: No — not necessarily.

Explanation:
- A Verifiability score = 1 means the verifiability dimension (as you defined it) reached its maximum according to the specific metrics and weights used in that composite score. It only guarantees that the metrics that are part of that linear combination have maximal (or weighted-maximal) values.
- It does not automatically mean ""all information about providers are set"" unless every provider-related metric (Author metadata/query, Publisher, Contributor, Sources, ""Is on a trusted provider list"", etc.) is included in the verifiability combination and each of those metrics has a maximal value.

How to be sure for a given dataset
1. Inspect which metrics belong to the Verifiability dimension (dqv:inDimension <http://example.org/Quality-Dimension/Verifiability>).
2. Check the latest measurement values for the provider-related metrics (Author metadata, Publisher, Contributor, Sources, Dataset URL, …) for that dataset. Only if all those metrics are present and have acceptable/maximal values can you conclude provider information is fully set.

Useful SPARQL (example) — lists the latest values of all metrics in the Verifiability dimension for a given dataset (replace the dataset IRI):
PREFIX dqv: <http://www.w3.org/ns/dqv#>
PREFIX prov: <http://www.w3.org/ns/prov#>
PREFIX skos: <http://www.w3.org/2004/02/skos/core#>

SELECT ?metric ?label ?value ?time WHERE {
  ?metric dqv:inDimension <http://example.org/Quality-Dimension/Verifiability> .
  OPTIONAL { ?metric skos:prefLabel ?label }
  {
    SELECT ?metric (MAX(?t) AS ?time)
    WHERE {
      ?qm a dqv:QualityMeasurement ;
          dqv:isMeasurementOf ?metric ;
          dqv:computedOn <http://example.org/dataset/NoiPA> ;
          prov:generatedAtTime ?t .
    } GROUP BY ?metric
  }
  ?qm a dqv:QualityMeasurement ;
      dqv:isMeasurementOf ?metric ;
      dqv:computedOn <http://example.org/dataset/NoiPA> ;
      dqv:value ?value ;
      prov:generatedAtTime ?time .
  FILTER(?time = ?time)  # (keeps the triple pattern clear; the subquery already provided ?time)
}
ORDER BY ?metric

Note: ""Is on a trusted provider list"" in your data is in the Believability dimension, so a Verifiability = 1 could be true while that trusted-provider metric is still False — check both dimensions if you need provider trust + verifiability.

If you want, I can:
- run a ready SPARQL (if you give the dataset IRI and access), or
- produce a short checklist of the exact provider-related metrics to verify for your setup.
",0,0,0,,0,,,,,,
E&D_5,"If the verifiability score is 1, does it mean that all provider information is available?",Yes,"Short answer: No — a verifiability score of 1 only means that the verifiability metric (as it was defined and computed) reached its maximum possible value. It does not automatically guarantee that ""all provider information"" exists in every possible form or every field you might expect.

Why:
- The Verifiability metric in your data is described as ""Overall score for the verifiability dimension computed as a linear combination of the individual verifiability metrics scores"". So a score of 1 means the individual verifiability sub-metrics used in that linear combination all attained their best values (given their own definitions, thresholds and weights).
- Which attributes are checked depends on which sub‑metrics were included in that verifiability dimension. For example, in your dataset the metric Sources is explicitly in the Verifiability dimension, while the metric Is on a trusted provider list is in the Believability dimension — so a perfect verifiability score would not tell you anything about the trusted‑provider list value unless that sub‑metric is part of the verifiability calculation.
- Some sub‑metrics are boolean, some return strings (e.g. source website/name/email), and some are scored numerically. A numeric 1 can mean ""all measured items passed the checks"" but it only covers the attributes the check actually inspects. Unmeasured or out‑of‑scope provider fields may still be missing.

How to be sure:
- Inspect the individual metrics that feed the Verifiability score and their values for the dataset in question. For example you can query the measurements that belong to the Verifiability dimension and check their values and timestamps.
- Example SPARQL you can run against your triple store to list verifiability metrics and measurements for a dataset (replace dataset IRI if needed):

PREFIX dqv: <http://www.w3.org/ns/dqv#>
PREFIX skos: <http://www.w3.org/2004/02/skos/core#>
PREFIX prov: <http://www.w3.org/ns/prov#>

SELECT ?metric ?label ?value ?time WHERE {
  ?metric a dqv:Metric; dqv:inDimension <http://example.org/Quality-Dimension/Verifiability>; skos:prefLabel ?label .
  ?mm a dqv:QualityMeasurement; dqv:isMeasurementOf ?metric; dqv:computedOn <http://example.org/dataset/NoiPA>; dqv:value ?value; prov:generatedAtTime ?time .
}
ORDER BY ?metric ?time

If you want, I can (a) list the specific verifiability sub‑metrics used here and their latest values for a dataset you name, or (b) help interpret any particular sub‑metric (e.g. Sources, Author (metadata), R1.2 Publisher) so you know which provider fields are actually covered. Which would you prefer?
",0,0,0.5,,0,,,,,,