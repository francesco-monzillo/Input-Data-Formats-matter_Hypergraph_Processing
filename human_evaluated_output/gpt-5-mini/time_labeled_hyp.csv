Question_ID,Request,Expected_Response,Actual_Response,Voter 1,Voter 2,Voter 3,Colonna 1,Final,,,,,,
IR_1,"Retrieve the SPARQL endpoint status of NoiPA in the date September 7th, 2025?",On,"On 2025-09-07 the NoiPA SPARQL endpoint was reported as: Available

- SPARQL endpoint URL: https://sparql-noipa.mef.gov.it/sparql
- Requires authentication: False",1,1,1,,1,,,,,,
IR_1,Is the NoiPA SPARQL endpoint up in the last assessment?,On,"Yes. In the last assessment (2025-09-07) the NoiPA SPARQL endpoint was up.

Evidence:
- Endpoint URL: https://sparql-noipa.mef.gov.it/sparql
- A1-D Working access point(s) = 1.0 (operational)
- Number of triples (query) = 446,170,057 (endpoint returned results)
- Requires authentication = False

If you want, I can run specific SPARQL queries or check current live status.",1,1,1,,1,,,,,,
IR_2,Which is the amount of data of the LiLa Lemma Bank dataset?,"1,699,687","Short answer — two ways to report it:

- Human description: the Lemma Bank contains approximately 200,000 canonical Latin forms.  
- Measured quantities in the assessment:
  - Amount-of-data score: 0.333333
  - Number of metadata triples: 1,699,687
  - Number of triples linked: 3,318,298

If you meant a different metric (e.g. total triples in the full KB, file size, or download links), tell me which and I’ll pull that.
",1,1,0.5,,1,,,,,,
IR_2,How many triples does LiLa Lemma Bank have?,"1,699,687","According to the metadata, the LiLa Lemma Bank contains 1,699,687 triples.  
(Recorded number of triples linked: 3,318,298.)",1,1,1,,1,,,,,,
IR_3,Which is the license of the DBLP dataset?,CC-0,"The DBLP Knowledge Graph is released under CC0 (Creative Commons Public Domain Dedication, CC0 1.0). 

Relevant links:
- CC0 deed: https://creativecommons.org/publicdomain/zero/1.0/
- OpenDefinition entry: http://www.opendefinition.org/licenses/cc-zero

(According to the dataset metadata and SPARQL-queryed metadata, the license is indicated as CC0.)",1,1,1,,1,,,,,,
IR_3,How is DBLP licensed?,CC-0,"The DBLP Knowledge Graph is released into the public domain under CC0 (CC0 1.0 Universal).  
Machine-readable license links found in the dataset metadata / SPARQL responses include:

- https://creativecommons.org/publicdomain/zero/1.0/  
- http://www.opendefinition.org/licenses/cc-zero

You can verify this in the DBLP KG metadata or via their SPARQL endpoint (https://sparql.dblp.org/sparql).
",1,1,1,,1,,,,,,
IR_4,Which is the accuracy score of WordNet?,0.00,"The accuracy score for WordNet (w3c-wordnet / WordNet 2.0 (W3C)) is 0.000000 (i.e., 0.0).
",1,1,1,,1,,,,,,
IR_4,How accurate is WordNet?,0.00,"It depends what you mean by “accurate.” Two short answers from the data you gave:

- Automated dataset-quality view (W3C RDF WordNet, audit timestamp 2025‑09‑07): Accuracy score = 0.0. In other words, the quality checker used here reports no positive signals for the accuracy dimension for that published RDF dataset. Other related findings in that same audit: Verifiability ≈ 0.332, Believability = 0.80, Completeness = 0.0, F1‑D (URI dereferenceability) = 0.0, Interlinking = 0.0, Availability/metadata checks (R / A items) are mostly positive (license and metadata retrievable). So the automated audit flags problems with dereferenceability, completeness and interlinking — not with provenance/metadata.

- Linguistic / lexical accuracy (how “correct” WordNet’s synsets and relations are): that is not measured by the dataset-quality metrics above. Princeton WordNet is a long‑standing, manually curated lexical resource widely used and generally considered high quality for many tasks, but it is not perfect — coverage and relation correctness vary by domain and sense granularity. If you need WordNet’s accuracy for a particular task (word–sense disambiguation, synonym lookup, semantic relatedness, etc.), the right approach is to benchmark it on task‑specific gold data (e.g., SemEval / sense‑tagged corpora) or sample and manually inspect synsets relevant to your domain.

If you want, I can:
- List the exact audit checks that drove the accuracy = 0.0 (the dump includes many low‑level metrics), or
- Run a small manual spot‑check of synsets/relations you care about, or
- Recommend benchmark datasets and evaluation procedures to measure WordNet’s usefulness for your task. Which would you prefer?
",1,1,1,,1,,,,,,
IR_5,What is the trust value of the Linked Open Camera dataset?,0.5,"The trust value for the Linked Open Camera dataset (bpr) is 0.50.
",1,1,1,,1,,,,,,
IR_5,How trustworthy is the Linked Open Camera dataset?,0.5,"Short answer: moderately trustworthy — overall a dependable institutional dataset (trust value 0.50) with excellent availability and security but poor interlinking and limited verifiability.

Key numbers (from the extracted metrics)
- Trust value: 0.50 (moderate)
- Security score: 1.00 (high)
- Availability:
  - Metadata available via primary sources (A1-M): 1.00
  - RDF dump available (metadata): 1.00
  - SPARQL endpoint: available (http://dati.camera.it/sparql)
- Representational‑consistency: 0.876 (good)
- Verifiability score: 0.11 (low)
- Interlinking score / completeness: 0.00 (none)
- Licensing: machine‑readable license present (CC BY‑SA indicated)
- Reputation score: very low (~8e‑6)
- Presence of example / documentation: present

What this means
- Strengths
  - Good provenance/metadata exposure and working access points (SPARQL, RDF dump), so you can retrieve and reuse the data easily.
  - Strong security posture and an explicit machine‑readable license (helps lawful reuse).
  - High representational consistency (the data uses consistent modelling), which eases processing and integration within applications.
- Weaknesses / risks
  - Very low interlinking with other datasets (no or negligible owl:sameAs/links). That limits the dataset’s usefulness in federated queries and reduces its ability to be automatically connected to other LOD sources.
  - Low verifiability and reputation scores — it’s not easy to independently verify claims in the data, and the dataset has almost no measurable community reputation.
  - Some provenance details are minimal (source web may be absent though a name/email is recorded), and automated checks (signing, richer provenance annotations) are lacking.

Practical recommendation
- If you need authoritative, downloadable data with good availability and security (e.g., for local processing, internal analysis, or applications that do not require dataset linking), this dataset is a solid choice.
- If you plan to integrate this dataset into a larger linked‑data graph, or rely on automatic joins across datasets, consider:
  - adding or requesting more interlinks (sameAs/seeAlso) to external datasets,
  - improving provenance metadata (signed metadata, explicit publisher URLs),
  - increasing verifiability (clear sources for critical facts) and promoting the dataset so its reputation grows.

If you want, I can:
- fetch and show the dataset’s license, example metadata, and the SPARQL endpoint status;
- suggest specific SPARQL queries to estimate interlinking or to extract provenance fields; or
- draft wording you could send to the dataset publisher asking for more interlinking and provenance info.
",1,1,1,,1,,,,,,
TA_1,Is the number of triples of NoiPA retrieved via the SPARQL endpoint always the same?,"No, they slightly increase over time.","No. The count returned by the SPARQL endpoint changes over time — it increases in the measurements you provided:

- 2025-05-04: 412,894,535 triples  
- 2025-07-13: 432,861,574 triples  
- 2025-08-03: 439,796,613 triples  
- 2025-09-07: 446,170,057 triples

(There is a separate static metadata number of 340,000,000 triples, but the live SPARQL query counts above differ and grow over time.)

Likely cause: the KG is being updated (historical updates are recorded), so the endpoint’s triple count is not constant. If you want to track this yourself, run a periodic SPARQL COUNT query (e.g. SELECT (COUNT(*) AS ?triples) WHERE { ?s ?p ?o }) against https://sparql-noipa.mef.gov.it/sparql.
",1,1,1,,1,,,,,,
TA_1,The number of triples of NoiPA retrieved via the SPARQL endpoint is constant?,"No, they slightly increase over time.","No — the triple count from the SPARQL endpoint is not constant; it increases over time. Key values from your logs:

- 2025-05-04: 412,894,535 triples  
- 2025-07-13: 432,861,574 triples  
- 2025-08-03: 439,796,613 triples  
- 2025-09-07: 446,170,057 triples

From 2025-05-04 → 2025-09-07 the dataset grew by 33,275,522 triples (~8.06%). Note also the metadata-stated count (Number of triples (metadata) = 340,000,000) is static/outdated compared with the live SPARQL counts. 

If you need, I can plot the trend, compute more precise growth rates, or show the SPARQL query used to get the counts.
",1,1,1,,1,,,,,,
TA_2,Is the SPARQL endpoint of DBLP always available?,Yes,"Short answer: No guarantee of being ""always"" available, but yes — the DBLP SPARQL endpoint (https://sparql.dblp.org/sparql) was reported reachable in all the recorded checks and shows high availability.

Details from the recorded checks
- Endpoint URL: https://sparql.dblp.org/sparql
- Status recorded as ""Available"" on: 2025-05-04, 2025-07-13, 2025-08-03 and 2025-09-07.
- Availability / monitoring metrics:
  - Availability score was high in the snapshots (e.g. 0.975 → 1.0 → 1.0 → 0.75 across different dates), indicating generally high uptime in the checks.
  - Number of triples (query) indicates a very large and changing dataset (e.g. ~1.33B → ~1.45B across snapshots), consistent with a maintained, live service.
  - Observed average latency ~0.1–0.2 s in those checks.
- RDF dumps: download links are provided (e.g. https://doi.org/10.4230/dblp.rdf.ntriples and https://dblp.org/rdf/dblp.ttl.gz). Metadata indicates dumps are available, though the endpoint itself does not expose RDF-dump download via a SPARQL query.

Practical note
- ""Always available"" is an absolute you cannot reliably assume for any publicly hosted service. The monitoring snapshots show the DBLP SPARQL endpoint is generally up and responsive, but outages or maintenance can occur.
- If your application depends on continuous access, add retry/backoff, caching, or a local copy (use the official RDF dump) and consider active monitoring (periodic ASK or small SELECT queries) to detect outages early.

If you want, I can:
- run a live check (ASK {?s ?p ?o} LIMIT 1) against the endpoint now, or
- provide a small robust client pattern (retry/backoff + caching) and an example ASK query to monitor availability.
",1,1,1,,1,,,,,,
TA_2,Is the DBLP SPARQL endpoint always up?,Yes,"Short answer: No — you can't assume it's ""always"" up. But in our checks the DBLP SPARQL endpoint (https://sparql.dblp.org/sparql) was reachable at the sampled dates (2025-05-04, 2025-07-13, 2025-08-03 and 2025-09-07) and responded to queries, so it appears reliably available in those observations.

Notes and practical advice
- Evidence: the endpoint was recorded as ""Available"" on the timestamps above and returned query results (large triple counts recorded), so it is normally up and responsive.
- However, any public service can have transient outages — you should not rely on 100% uptime without your own monitoring or SLA.
- If the SPARQL endpoint is unavailable you can use the downloadable RDF dumps:
  - https://doi.org/10.4230/dblp.rdf.ntriples
  - https://dblp.org/rdf/dblp.ttl.gz

Quick availability test (one-line)
- ASK query (fast):
  curl -H ""Accept: application/sparql-results+json"" --get --data-urlencode 'query=ASK { ?s ?p ?o }' https://sparql.dblp.org/sparql

- HEAD-request to check HTTP reachability:
  curl -I https://sparql.dblp.org/sparql

Recommendations
- Add simple periodic checks (ASK or HTTP HEAD) and alerting to detect outages.
- Cache critical query results or download the RDF dump if you need guaranteed continuous local access.
",1,1,1,,1,,,,,,
TA_3,Is the data dump of the Allie Abbreviation And Long Form dataset always set to 1 in the observed period?,"No, it is discontinuous","No — not always. The recorded values for ""Availability of RDF dump (metadata)"" are:

- 2025-05-04: 1.000000  
- 2025-06-01: 1.000000  
- 2025-07-13: 1.000000  
- 2025-08-03: 1.000000  
- 2025-09-07: 0.000000

So the dump was available (1) for the first four observations but is 0 on 2025-09-07. If you want, I can show other related measures (e.g., offline dump URLs, ""Availability of RDF dump (query)"", metadata media types) to help diagnose why it changed.
",1,1,1,,1,,,,,,
TA_3,Is the data dump of the Allie Abbreviation And Long Form dataset always up?,"No, it is discontinuous","Short answer: No — not always.

Details:
- There is an RDF dump listed (offline dumps / metadata) at:
  ftp://ftp.dbcls.jp/allie/allie_rdf/allie_rdf_nt_latest.gz
  (also the parent directory ftp://ftp.dbcls.jp/allie/allie_rdf/).
- However automated checks flagged inactive links / downtime (Inactive links = True) and the dataset’s ""availability for download (query)"" was reported False in monitoring runs. Availability scores are below 1 (around 0.73–0.74), which indicates intermittent unavailability.
- The dataset does provide other access points you can try: the dataset homepage http://allie.dbcls.jp/ and a SPARQL endpoint at http://data.allie.dbcls.jp/sparql.

If you need reliable access, I recommend:
- Trying to download the FTP file directly (wget or curl) and retrying if it fails.
- Using the SPARQL endpoint as a fallback.
- Mirroring the dump to your own storage when it is available.
",1,1,1,,1,,,,,,
TA_4,Is the accuracy of NoiPA constant over time?,No,"Not exactly — it's effectively constant but shows a very small drop. Recorded values:

- 2025-05-04: Accuracy = 0.999700  
- 2025-07-13: Accuracy = 0.999660  
- 2025-08-03: Accuracy = 0.999660  
- 2025-09-07: Accuracy = 0.999660

Change = 0.000040 (absolute), ≈0.004% relative — negligible in practice, but not strictly identical over time.
",1,1,1,,1,,,,,,
TA_4,Does the accuracy of NoiPA remain consistent over time?,No,"Short answer: Yes — the accuracy is essentially unchanged.

Details:
- 2025-05-04: Accuracy = 0.999700
- 2025-07-13: Accuracy = 0.999660
- 2025-08-03: Accuracy = 0.999660
- 2025-09-07: Accuracy = 0.999660

Absolute change from the highest (0.999700) to the later values (0.999660) is 0.00004 (≈0.004% relative), which is negligible. The score remains near 0.99966–0.99970 across the recorded dates, so accuracy is stable over time.

If you want, I can:
- plot the trend,
- compute statistical significance,
- or compare accuracy stability to other datasets/metrics.
",1,1,1,,1,,,,,,
TA_5,What is the longest continuous period in which the Allie Abbreviation And Long Form dataset data dump has been equals to 1?,91 days,"I checked the ""Availability of RDF dump (metadata)"" measure for the Allie dataset. The measure equals 1 at these consecutive timestamps: 2025-05-04, 2025-06-01, 2025-07-13 and 2025-08-03, and then is 0 (or absent) at 2025-09-07.

Therefore the longest continuous period with value = 1 is from 2025-05-04 to 2025-08-03 inclusive — a span of 92 days.
",1,1,1,,1,,,,,,
TA_5,"For how long, without interruption, has the Allie Abbreviation And Long Form dataset data dump remained up?",91 days,"I can't determine that from the available measurements — there is no recorded uptime/downtime history or a numeric ""time up without interruption"" for the Allie dataset in the data you provided. The dataset metadata shows offline dump URLs (ftp://ftp.dbcls.jp/allie/allie_rdf/allie_rdf_nt_latest.gz) and metadata indicates a dump is available, but the ""time elapsed since last modification"" / continuous-uptime fields are empty (""-"") in the records.

How you can get the uninterrupted-uptime information (practical options)

1) Quick check of the dump file's last-modified timestamp (FTP)
- Use an FTP listing to see the file timestamp (this tells when the dump was last changed, not how long it has been continuously accessible):
  - lftp:
    lftp -c ""open ftp://ftp.dbcls.jp; ls -l allie/allie_rdf/allie_rdf_nt_latest.gz""
  - curl (may work depending on server):
    curl -I ftp://ftp.dbcls.jp/allie/allie_rdf/allie_rdf_nt_latest.gz

2) Check HTTP/HEAD on download or SPARQL endpoints (returns Last-Modified or headers)
- For the dump (if served over HTTP) or dataset homepage:
  curl -I http://allie.dbcls.jp/
- For the SPARQL endpoint:
  curl -I http://data.allie.dbcls.jp/sparql

3) Verify continuous access historically (requires monitoring logs or external uptime monitoring)
- To know ""for how long, without interruption"", you need either:
  - server logs (web/ftp server access logs) showing no outages, or
  - an uptime-monitoring record (UptimeRobot, Pingdom, Prometheus alerts) that was running against the dump URL/endpoint.
- If you have access to the server or to historical monitoring, compute the contiguous interval(s) with successful responses and report the longest one.

4) If you only need an approximation from available metadata
- Use the file's last-modified timestamp and assume the dump has been served continuously since that timestamp (only an assumption — not proof of uninterrupted availability).

If you want, I can:
- Run the curl/lftp HEAD commands for you (if you give me permission to access the dump URL from here), or
- Provide a short script to run periodically to record uptime and compute uninterrupted availability going forward. Which would you prefer?
",0,0,0,,0,,,,,,
Co_1,Which dataset has the highest number of triples?,DBLP,"The dblp-kg dataset has the most triples — about 1,452,862,425 triples (≈1.45 billion) (measure value 1,452,862,425 at 2025-09-07).
",1,1,1,,1,,,,,,
Co_1,Which is the biggest dataset among the compared ones?,DBLP,"The dblp-kg is the largest: about 1.3–1.45 billion triples.  
- Example reported values for dblp-kg (Number of triples (query)): 1,335,106,894; 1,448,894,292; 1,452,862,425 (≈1.45B).  

For comparison: NoiPA ≈ 340–446M triples; bpr ≈ 353–354M; allie ≈ 303–309M; LemmaBank ≈ 1.7M.
",1,1,1,,1,,,,,,
Co_2,Which is the dataset performing worst in the license metric?,CIDOC-CRM,"CIDOC-CRM — Licensing score = 0.000000 (worst among the datasets; others are 0.5 or 1.0).
",1,1,1,,1,,,,,,
Co_2,Which is the worst performing dataset in the license metric?,CIDOC-CRM,"CIDOC-CRM — Licensing score 0.0.

Reason: its license is not detected (license machine-readable metadata = False / license fields missing), so it scores 0 on the licensing metric. To improve, add an explicit human- and machine‑readable license URI in the dataset metadata (e.g., VoID/DCAT) and on the dataset landing page.
",1,1,1,,1,,,,,,
Co_3,What is the mean score for the accuracy dimension?,0.50,"Using the latest Accuracy score values present for each dataset (w3c-wordnet 0.000000; bpr 1.000000; NoiPA 0.999660; LemmaBank 0.000000; CIDOC-CRM 0.000000; dblp-kg 1.000000; allie-abbreviation-and-long-form-database-in-life-science 1.000000; micro-coronavirus 1.000000; bbc-programmes 0.000000), the mean accuracy score is:

Mean = (0 + 1 + 0.999660 + 0 + 0 + 1 + 1 + 1 + 0) / 9 ≈ 0.55552

Rounded: approximately 0.556 (55.6%).",0,0,0,,0,,,,,,
Co_3,What is the mean value of the accuracy dimension?,0.50,"I computed the mean Accuracy score using the latest Accuracy values per dataset present in the log. Datasets and latest Accuracy values used:

- w3c-wordnet: 0.000000  
- NoiPA: 0.999660  
- bpr: 1.000000  
- dblp-kg: 1.000000  
- LemmaBank: 0.000000  
- allie-abbreviation-and-long-form-database-in-life-science: 1.000000  
- bbc-programmes: 0.000000  
- CIDOC-CRM: 0.000000  
- micro-coronavirus: 1.000000

Sum = 4.999660; count = 9  
Mean = 4.999660 / 9 ≈ 0.555518

So the mean value of the accuracy dimension (across those datasets) is approximately 0.5555 (≈0.556). If you want the mean computed differently (e.g., including every timestamped measure separately), tell me and I’ll recompute.
",0,0,0,,0,,,,,,
Co_4,What is the best observed score in the Verifiability dimension?,"0.4983 (NoiPA, Allie, Bathing Water Quality)","The highest Verifiability score observed is 0.498333. 

This value appears for several datasets (e.g., NoiPA, allie-abbreviation-and-long-form-database-in-life-science, environment-agency-bathing-water-quality) at various timestamps.
",1,1,1,,1,,,,,,
Co_4,What is the highest observed Verifiability score?,"0.4983 (NoiPA, Allie, Bathing Water Quality)","The highest Verifiability score observed is 0.498333.

Datasets with that score include (examples):
- NoiPA (0.498333 at multiple timestamps)
- allie-abbreviation-and-long-form-database-in-life-science (0.498333)
- environment-agency-bathing-water-quality (0.498333)",1,1,1,,1,,,,,,
Co_5,What are the highest-performing dimensions in September 2025?,"License, Performance, Accuracy, Believability, Currency, Timeliness, Security, Conciseness and Interpretability","Looking at the 2025-09 snapshot (entries dated 2025-09-07) in your data, the dimensions that stand out as highest‑performing (many metrics at or near the maximum) are:

- Metadata & discoverability
  - Multiple metadata/discoverability metrics reach 1.0 for several datasets: A1-M (metadata via working primary sources), A2-M (registered in search engines), F2a-M (metadata available via standard sources) and F4-M (metadata registered in a searchable engine) — e.g. bpr, dblp-kg, w3c-wordnet, CIDOC-CRM and NoiPA show 1.0 on one or more of these measures.

- Availability / Accessibility (including download / endpoints / HTTPS)
  - Availability-for-download (metadata) = 1.0 for bpr, CIDOC-CRM, dblp-kg, w3c-wordnet.
  - Availability of RDF dump (metadata) = 1.0 for several datasets.
  - A1.2 (Authentication & HTTPS support) = 1.0 for bpr, dblp-kg, NoiPA; bpr also reports Use HTTPS = True.
  These indicate strong accessibility/security support across multiple KGs.

- Identifier & referencability
  - F1-M Unique & persistent ID = 1.0 (w3c-wordnet).
  - F3-M (data referrable via DOI) = 1.0 for CIDOC-CRM.
  - URIs dereferenceability = 1.0 for dblp-kg.
  These metrics imply good identifier practices on several KGs.

- Conciseness / Representational quality
  - Intensional/extensional conciseness = 1.0 for some datasets (e.g., dblp-kg, NoiPA), and conciseness scores are high for others (bpr ≈ 0.884; NoiPA ≈ 0.991).
  - Representational-consistency and related scores are also high for some datasets.

- Performance (for some datasets)
  - Performance score = 1.0 for dblp-kg and NoiPA; bpr ~0.507; micro-coronavirus ~0.505. So performance is dataset‑dependent but top for dblp-kg/NoiPA.

Other dimensions (trust/believability, verifiability, availability, licensing) show moderate to high values in places (trust ~0.75 for several KGs; licensing 0.5 for some), but not as consistently maximal as the metadata / availability / identifier dimensions above.

If you want I can:
- produce a ranked list of dimensions by average score across datasets for 2025-09-07, or
- show per-dataset top dimensions (which dimension each dataset scores best on) for that date. Which would you prefer?
",0,0,0,,0,,,,,,
Co_5,What are the dimensions performing better in September 2025?,"License, Performance, Accuracy, Believability, Currency, Timeliness, Security, Conciseness and Interpretability","Short answer — at the 2025-09-07 snapshot the strongest (best-performing) quality dimensions are:

- Availability (best).  
  - Repeatedly high/positive measures across KGs: A1/A2 metadata availability true, Availability-for-download (metadata) = 1.0, RDF-dump availability = 1.0 for several datasets (bpr, dblp-kg, w3c-wordnet, NoiPA).  
- Trust / Believability (good).  
  - Many datasets show comparatively high trust/believability scores (e.g., dblp-kg: trust 0.75; w3c-wordnet: trust 0.75; NoiPA and several others show 0.75—1.0 for related trust/ R scores).  
- Licensing (moderate).  
  - Licensing score commonly ~0.5 across datasets (so not excellent but consistently present: R1.1/R1.2 licensing/ publisher metadata are generally available).  
- Security (mixed-to-good).  
  - Some datasets show strong security measures (bpr security = 1.0; many others score 0.5), so security is reasonable overall for a number of KGs.  
- Conciseness / Representational consistency (dataset-dependent but often good).  
  - Several datasets show high conciseness/representational-consistency (e.g., NoiPA, bpr, dblp-kg show strong conciseness/representational scores).

Dimensions performing poorly in the same snapshot
- Currency (often low or zero) — several datasets have 0.0 currency / volatility scores (w3c-wordnet, dblp-kg, many others).  
- Interlinking (low) — interlinking/interlinking completeness scores are generally near 0 for many datasets.  
- Performance (varies; several datasets report 0 or missing performance scores).  
- Interpretability/Understandability (mixed; some datasets low).

If you want an exact ranked list (e.g., mean or median score per dimension across all datasets at 2025-09-07), I can compute that from the raw measures and return a numeric ranking. Which aggregation (mean, median, weighted) would you prefer?
",0,0,0,,0,,,,,,
E&D_1,Why is the availability score equals to 1 for the DBLP dataset?,Because all the metrics in the availability dimension can be correctly computed,"Short answer: because the DBLP KG meets all the availability checks used to compute that dimension (working access point, downloadable RDF dump/metadata, searchable registration, machine-readable license, etc.), so the weighted sum of those availability metrics equals 1.

Evidence from the provided measurements (dblp-kg):
- A1-D Working access point(s) = 1.0 (SPARQL endpoint URL: https://sparql.dblp.org/sparql)
- Availability of RDF dump (metadata) = 1.0
- Availability for download (metadata) = 1.0
- F2a-M / F2b-M (metadata availability via standard/covered attributes) are high (F2a-M = 1.0; F2b-M ≈ 0.78)
- A2-M Registered in search engines = 1.0
- F4-M Metadata registered in a searchable engine = 1.0
- R1.1 Machine- or human-readable license retrievable = 1.0 (license machine‑readable: https://creativecommons.org/publicdomain/zero/1.0/ and metadata license: http://www.opendefinition.org/licenses/cc-zero)
- F3-M Data referrable via a DOI = 1.0 (download URLs include https://doi.org/10.4230/dblp.rdf.ntriples and https://dblp.org/rdf/dblp.ttl.gz)

Note: some query-based availability flags (e.g., ""Availability of RDF dump (query)"" / ""Availability for download (query)"") are reported False or missing in the log, but the availability score is computed from a set of availability-related metrics (including metadata-level checks and registration), and because those key metrics are satisfied the resulting availability score is 1.

If you want, I can:
- show the exact availability metrics used and their numeric contributions, or
- compute the availability score from the component values using the formula/weights you use.
",0,0,0.5,,0,,,,,,
E&D_1,Why does the DBLP dataset have an availability score of 1?,Because all the metrics in the availability dimension can be correctly computed,"The availability score is 1.0 because all the main availability indicators for dblp-kg are satisfied:

- A working SPARQL endpoint is online (SPARQL endpoint URL: https://sparql.dblp.org/sparql — A1-D = 1.0).  
- An RDF dump is provided and available for download (Availability of RDF dump (metadata) = 1.0 and Availability for download (metadata) = 1.0). The reported download URLs are: https://doi.org/10.4230/dblp.rdf.ntriples and https://dblp.org/rdf/dblp.ttl.gz.  
- Metadata are published via standard primary sources and registered in searchable engines (F2a-M = 1.0, F2b-M ≈ 0.78, A2-M = 1.0), so the dataset is discoverable and its metadata accessible.

(“Availability of RDF dump (query)” being False only means the dump cannot be retrieved via a SPARQL query endpoint; it does not negate that the dump files and a working endpoint exist. Because the dataset has at least one working access point plus downloadable RDF dumps and registered metadata, the combined availability metric reaches the maximum value.)
",0,0,0.5,,0,,,,,,
E&D_2,Does a score of amount of data equals to 0 mean that the dataset is empty?,"No, it means that the amount of data cannot be computed via the SPARQL endpoint and it is not declared in the metadata.","Not necessarily. A 0 for the ""Amount of data"" score means the computed, normalized combination of the underlying amount metrics equals 0 — not that it has literally zero triples. There are a few common reasons the combined score can be 0:

- All contributing metrics are zero (e.g., no triples, no entities, no labels/comments). In that case the dataset really has no measured content.  
- One or more required inputs are missing (SPARQL endpoint not available, VoID/metadata absent) so query-based metrics default to 0 or are not computed, which can drive the combined score to 0 even if offline dumps exist.  
- The score is produced by a normalization/thresholding scheme: counts below a configured minimum map to 0. A small dataset can therefore get 0 even though it contains some triples.  
- A measurement error (query failures, access/permission problems, malformed metadata) made the implementer set the metric to 0.

How to check what happened
1. Inspect the raw metrics that feed the ""Amount of data"" score: number of triples (query), number of triples (metadata), number of entities, number of labels/comments. If these are > 0, the scoring mapping/thresholds are the likely cause.  
2. Verify SPARQL endpoint availability and run quick counts:
   - Count triples: SELECT (COUNT(*) AS ?triples) WHERE { ?s ?p ?o }  
   - Count distinct subjects/entities: SELECT (COUNT(DISTINCT ?s) AS ?entities) WHERE { ?s ?p ?o }  
   (Run these on the dataset's SPARQL endpoint or load the dump into a triplestore.)  
3. Check metadata/VoID and “Number of triples (metadata)” — there may be a dump or metadata count even if the endpoint is missing.  
4. Review logs/errors from the assessment tool to see if queries failed or inputs were missing.  
5. If the scoring uses thresholds, ask/inspect how counts are mapped to the [0,1] range (min/max or step thresholds).

If you want, tell me which dataset entry you care about from your data dump and I can look at the relevant raw metric lines and suggest the likely cause and concrete next checks.
",0.5,0,0.5,,"0.5",,,,,,
E&D_2,"If the amount-of-data score is 0, does that mean the dataset contains no data?","No, it means that the amount of data cannot be computed via the SPARQL endpoint and it is not declared in the metadata.","Short answer: No — a 0 amount-of-data score usually means “no measurable / no detected / below‑threshold data according to the measurement method”, not necessarily that the dataset is literally empty.

Why not necessarily empty
- Measurement method: the score is a normalized/composite metric (counts, thresholds and weights). If the measured quantities (triples, entities, labels, etc.) are missing, below the method's minimum, or outside the range used for normalization, the score can be 0.
- Missing endpoints or metadata: if the SPARQL endpoint is down, inaccessible, or the VoID/metadata is absent, automated checks often report 0 because they cannot retrieve counts.
- Only metadata present: sometimes only metadata exists (or only a small RDF stub), so the scoring routine finds no content to count and returns 0.
- Errors / sampling: errors in queries, access restrictions, or sampling rules (e.g., only counting certain predicate types) can lead to a 0 result even when some data exists.

How to confirm whether the dataset is actually empty
1. Check the raw triple/entity counts reported in metadata:
   - Look for fields like ""Number of triples (metadata)"", ""Number of triples (query)"", ""Number of entities"", ""Number of labels/comments present"".
2. Query the SPARQL endpoint directly (if available):
   - Total triples: SELECT (COUNT(*) AS ?triples) WHERE { ?s ?p ?o }
   - Distinct subjects/entities: SELECT (COUNT(DISTINCT ?s) AS ?entities) WHERE { ?s ?p ?o }
   - Count labels: SELECT (COUNT(DISTINCT ?s) AS ?labeled) WHERE { ?s rdfs:label|rdfs:comment ?label }
   - If endpoint is down or returns errors, that explains a 0 score.
3. Check for an RDF dump or VoID file and inspect it (download and check file size / triple counts).
4. Inspect metadata fields recorded in your dataset report (e.g., availability, R1.3 flags, Number of triples (metadata) — these sometimes show nonzero counts even when the amount-of-data score is 0).
5. Look at logs / error messages from the scoring tool to see whether the measurement failed (timeouts, authentication required, query errors).

What to do if you need a reliable amount-of-data measurement
- Ensure the SPARQL endpoint is reachable and permits counting queries.
- Provide an RDF dump or VoID with triple/entity counts.
- Check the scoring configuration/thresholds (what counts as “data” for that score) and adjust if needed.
- Re-run the measurement after fixing access or metadata problems.

If you want, tell me which dataset (from your data snippet) you’re asking about and I’ll point to the specific fields you can check (e.g., ""Number of triples (metadata)"" or ""Number of triples (query)"") and give the exact SPARQL queries to run.
",0.5,0,0.5,,"0.5",,,,,,
E&D_3,"When the availability score is 1, does it mean that both the SPARQL endpoint and the data dump are working?",Yes,"Short answer: Not necessarily.

Explanation:
- The ""availability score"" is a composite (linear combination) of several availability sub-metrics (e.g. A1‑D Working access point(s), Availability for download (metadata), Availability of RDF dump (metadata), Availability of RDF dump (query), Availability for download (query), SPARQL endpoint status, Offline/online dumps, etc.).
- A value of 1 means the composite reached its maximum given the sub-metrics used in the calculation — i.e. all the availability conditions that the scoring formula includes were met. Whether those conditions include both a working SPARQL endpoint and a downloadable RDF dump depends on the exact sub-metrics and weights used by your scoring implementation.

How to be certain:
1. Inspect the availability sub-metrics for that dataset (look for items named like A1‑D, ""Availability of RDF dump (metadata/query)"", ""SPARQL endpoint"", ""Availability for download"", etc.). If those individual measures are all 1 / “Available”, then both the SPARQL endpoint and the dump are available.
2. Or test manually:
   - SPARQL endpoint: run a simple query or an HTTP HEAD/GET against the endpoint (e.g. curl -I ""https://example.org/sparql"" or curl --data-urlencode ""query=ASK{?s ?p ?o}"" -G ""https://example.org/sparql"").
   - RDF dump: request the dump URL (HEAD/GET) to confirm it can be downloaded (e.g. curl -I ""https://example.org/dump.nt.gz"").

If you paste the specific dataset entry or the sub-metrics for that dataset here, I can check which availability fields were satisfied and tell you whether both the endpoint and dump are actually working.
",0,0,0,,0,,,,,,
E&D_3,"if the availability score is 1, does that mean both the SPARQL endpoint and the data dump are up?",Yes,"Short answer: not necessarily.

Explanation and what to check
- The ""availability score"" is an aggregate (a weighted combination) of several availability metrics (e.g., presence of working access points, metadata saying a dump is available, VoID/DCAT metadata, SPARQL endpoint responsiveness, download URLs). A score of 1 means the dataset scored “perfect” according to those metrics, but it does not by itself tell you which exact access modes are live right now.
- In other words, a score of 1 usually indicates the dataset passed all the availability checks the tool uses — but those checks can include metadata claims (a dump URL recorded in metadata) rather than a live download test. So it’s possible for the score to be 1 while one specific channel (SPARQL or a dump) is actually down if the scoring relies on other indicators or metadata.

How you can verify concretely
1. Check the SPARQL endpoint field in the report and then run a simple query against it (e.g., SELECT ?s WHERE { ?s ?p ?o } LIMIT 1 or ASK {}) — or just curl the endpoint URL to see if it responds.
2. Look for ""URL for download the dataset"" / ""Offline dumps"" / ""Offline dumps (metadata)"" / ""Availability of RDF dump (query/metadata)"" in the report. Try to download the listed dump file(s) or open the dump URL.
3. Inspect VoID/DCAT metadata fields (if present) — they may indicate a dump exists; but verify the actual file is accessible.
4. Check ""A1-D Working access point(s)"" and other individual availability measures in the report to see which components produced the positive score.

If you want, tell me the dataset name (or point me to the SPARQL URL / dump URL from your report) and I can run the checklist steps (what to query and how) or give exact commands to test endpoint and download availability.
",0,0,0,,0,,,,,,
E&D_4,"if license is set to 1, does it mean that an open license is used?","No, it means that a license is specified","Short answer: No. A value of 1 for the license metric only means a license was reported and retrievable (machine- or human-readable) from a primary source — it does not by itself mean the license is ""open"".

Why not
- The metric R1.1 (or ""license present"") is a presence/retrievability check: 1 = a license string/URI was found; 0 = none found.
- Openness is a separate property of the specific license (what permissions, restrictions and requirements it contains). A reported license can be permissive/open (e.g., CC0, CC-BY, Apache-2.0), or it can be restrictive (e.g., CC BY-NC, CC BY-ND) or even ambiguous/custom.

How to determine whether a reported license is open
1. Retrieve the license identifier/URI (from the metadata or SPARQL query result).
2. Compare it against a whitelist of known open licenses (examples that are widely considered ""open""):
   - Public domain / very permissive: CC0-1.0 (CC0), Unlicense, public domain declarations
   - Permissive/open-source style: CC-BY-* (attribution), Apache-2.0, MIT, BSD
   - Open with conditions allowed by the Open Definition: share‑alike (e.g., CC BY-SA) is still considered open
3. Exclude licenses with restrictions incompatible with openness: NC (non-commercial), ND (no derivatives), or similar limitations. Jurisdictional/custom licenses need manual review.
4. If the license URI is unknown or custom, fetch the license text and check whether it grants reuse, redistribution and derivative works (and whether it imposes forbidden restrictions).
5. Watch out for scope issues: sometimes metadata uses one license but the actual data or third-party content within the dataset is under different terms — verify license applies to the data you want to use.

Ways to automate
- Maintain a mapping of license URIs/SPDX identifiers to “open / not open / needs review” (SPDX IDs and the Open Definition lists are useful sources).
- If metadata contains a machine-readable license URI (e.g., http://creativecommons.org/licenses/by/4.0/ or https://opensource.org/licenses/Apache-2.0 or https://creativecommons.org/publicdomain/zero/1.0/), map that to your openness rules.
- For unknown URIs, download the license page/text and search for keywords or run a human review.

Example: the metric = 1
- dataset license metric = 1 and license URI = http://creativecommons.org/licenses/by/4.0/ → license present and is CC-BY-4.0 (an open license).
- dataset license metric = 1 but license URI = http://creativecommons.org/licenses/by-nc/4.0/ → license present but NOT open (NC forbids commercial reuse).

If you want, I can:
- check a specific dataset entry from your data and tell you whether the reported license appears open (based on its URI/SPDX id), or
- give a small script or mapping list you can use to automatically classify reported licenses as open / non-open / unknown.
",1,1,1,,1,,,,,,
E&D_4,"If license is set to 1, does it mean that an open license is used?","No, it means that a license is specified","No — a value of 1 for the license metric means a license statement was found and is retrievable (machine- or human‑readable) from a primary source. It does not by itself mean the license is ""open"".

What you should do to determine whether the license is open
- Extract the license identifier/URI that the metric returned (e.g. http://creativecommons.org/licenses/by/4.0/, https://www.apache.org/licenses/LICENSE-2.0, etc.).  
- Resolve the URI or look up the identifier (SPDX id, Creative Commons URL, Open Government Licence, etc.).  
- Check the license terms against an openness reference (Open Definition / Open Knowledge Foundation, SPDX license list) to see if it permits unrestricted reuse, redistribution and modification (and to discover any conditions such as attribution, share‑alike, non‑commercial, or patent clauses).

Notes
- Many common licenses returned by your records are open (CC0, CC BY, Apache 2.0, MIT, OGL, etc.), but some licences with restrictions (e.g. NC, ND, proprietary “all rights reserved”) are not open.  
- Share‑alike (CC BY‑SA) is considered “open” under the Open Definition but imposes compatibility constraints when combining data.  
- If you want an automated test, map the returned license URI or SPDX identifier to a whitelist/registry of open licenses (SPDX / Open Definition lists) and flag those not on the list.

If you share one of the license URIs from your dataset I can check it and tell you whether that specific license is considered open and what reuse conditions apply.
",1,1,1,,1,,,,,,
E&D_5,"If verifiability is set to 1, does it mean that all information about providers are set?",Yes,"Short answer: No.

A verifiability score of 1 means ""all the verifiability checks that the evaluator computes were satisfied"" — i.e. every verifiability metric the tool measures returned the best (ideal) value. It does not automatically mean every possible piece of provider information exists or that the provider's identity has been independently validated.

Key points and caveats:
- What ""1"" covers depends on the specific metrics included in the verifiability dimension (examples in your dataset include: presence of author/publisher/contributor metadata, machine- and human-readable license, availability of VoID/DCAT metadata or SPARQL endpoint, whether the dataset is signed, presence on a trusted provider list, etc.). If the tool measures those and they are all present, verifiability → 1.
- It is bounded by the evaluator’s scope and data sources. If the evaluator does not check some provider fields (or cannot access a SPARQL endpoint/VoID), those fields won't affect the score.
- Verifiability = 1 is about traceability and the existence/retrievability of supporting metadata, not a guarantee of factual correctness or legal/identity validation. Identity verification of the provider (e.g., verifying the e-mail, confirming a DOI registration agency entry, or external accreditation) is typically outside the automatic verifiability metrics unless explicitly implemented.
- Scores can be optimistic if metadata are present but incorrect or stale. Always inspect the actual metadata entries (dcterms:creator, dcterms:publisher, contact, license, VoID/DCAT entries, signatures, etc.) rather than relying solely on the numeric score.

If you want, I can:
- List exactly which verifiability sub-metrics this tool uses and what each requires to be marked “pass,” or
- Inspect a specific dataset's metadata (give me the dataset id or metadata dump) and show which provider fields are present and which checks passed.
",0,0,0.5,,0,,,,,,
E&D_5,"If the verifiability score is 1, does it mean that all provider information is available?",Yes,"Short answer: Not necessarily.

Explanation:
- A verifiability score of 1 means that, for the set of verifiability metrics the evaluation tool checks, each metric achieved its maximum value. In other words, every verifiability test included in the score returned the expected positive result.
- It does not automatically guarantee that every possible piece of provider information exists or is correct. Reasons:
  - The score only covers the specific metrics the tool measures (license present, publisher/publisher metadata retrievable, DOI, machine-readable metadata, etc.). If the tool does not test a particular field (for example a contact email or an extended contributor list), those can still be missing.
  - Metadata could be present but ambiguous, incomplete, incorrect, or outdated; the tests typically detect presence/format, not semantic correctness or completeness.
  - Automated extraction/parsing can produce false positives (e.g., an html page that contains a license string that is not actually the dataset license).
  - Some checks may succeed because metadata is discoverable through a secondary source even if the dataset’s primary metadata is incomplete.

What to do to be certain the provider information is fully present:
- Inspect the dataset metadata (VoID/DCAT, Dataset/Distribution records) and confirm key fields: publisher, contributors, author, contact point, source(s), persistent identifier (DOI), license (human- and machine-readable).
- Check the dataset landing page/VoID file and the SPARQL endpoint (if any) for the same fields.
- Verify the details are correct (valid emails/URLs, institution names) and up-to-date.
- If you need a formal guarantee (e.g., for provenance or legal reasons), prefer human review in addition to the automated score.

Summary: Score = 1 indicates all tested verifiability checks passed, but you should still inspect the metadata to confirm that all provider information you care about is actually present and correct.
",0,0,0,,0,,,,,,