Question_ID,Request,Expected_Response,Actual_Response,Voter 1,Voter 2,Voter 3,COMMENTS,Final,,,,,,
IR_1,"Retrieve the SPARQL endpoint status of NoiPA in the date September 7th, 2025?",On,"On 2025-09-07 the NoiPA SPARQL endpoint was reported as: Available.

Details from the 2025-09-07 assessment:
- Endpoint URL: https://sparql-noipa.mef.gov.it/sparql
- Median latency: 0.113 (reported)
- Median throughput: 10.0 requests/sec

Note: the same report flags RDF dumps as offline/missing and URI dereferenceability as 0.0.
",1,1,1,,1,,,,,,
IR_1,Is the NoiPA SPARQL endpoint up in the last assessment?,On,"Yes — the most recent NoiPA assessment (2025-09-07) reports the SPARQL endpoint as available: https://sparql-noipa.mef.gov.it/sparql. 

Note: the reports also flag some issues (e.g., RDF dumps reported as missing/offline and URIs dereferenceability = 0.0), even though the endpoint itself is reachable.
",1,1,1,,1,,,,,,
IR_2,Which is the amount of data of the LiLa Lemma Bank dataset?,"1,699,687","The LiLa Lemma Bank contains about 1,699,687 triples (≈1.70 million triples) according to its metadata. Note: the SPARQL endpoint was reported missing/offline, so this metadata value could not be independently verified by query.
",1,1,1,,1,,,,,,
IR_2,How many triples does LiLa Lemma Bank have?,"1,699,687","LiLa Lemma Bank contains 1,699,687 triples (about 1.7 million triples).
",1,1,1,,1,,,,,,
IR_3,Which is the license of the DBLP dataset?,CC-0,"The DBLP dataset is licensed under CC0 (Creative Commons Zero 1.0 Universal Public Domain Dedication).  
Machine-readable references found: http://www.opendefinition.org/licenses/cc-zero and https://creativecommons.org/publicdomain/zero/1.0/
",1,1,1,,1,,,,,,
IR_3,How is DBLP licensed?,CC-0,"The DBLP RDF/dataset is released under CC0 (Creative Commons Zero — public domain dedication). Official references:

- CC0 1.0 Universal (human- and machine-readable): https://creativecommons.org/publicdomain/zero/1.0/  
- Open Definition label: http://www.opendefinition.org/licenses/cc-zero

This means the data is dedicated to the public domain and may be reused without copyright conditions (attribution is typically appreciated though not legally required).
",1,1,1,,1,,,,,,
IR_4,Which is the accuracy score of WordNet?,0.00,"The accuracy score for WordNet 2.0 (W3C) is 0.0 (0%). The reports attribute this to missing/unsupported SPARQL access, which prevented the intrinsic accuracy checks.
",1,1,1,,1,,,,,,
IR_4,How accurate is WordNet?,0.00,"Short answer
- For everyday English lexical relations (nouns, common verbs, adjectives, adverbs) WordNet is high‑quality: manually curated by lexicographers, widely vetted, and perfectly adequate for many NLP and lexicographic tasks.  
- For fine‑grained sense distinctions, rare senses, slang, proper names or technical/domain vocabulary, WordNet is incomplete or problematic — sense distinctions can be overly fine and some domain items are missing.  
- In practice WordNet is very useful, but you should be aware of its limitations and often combine it with corpus evidence or domain resources.

Why people trust WordNet
- It is a hand‑built lexical resource from Princeton University (standard reference for English lexical semantics), not an automatically inferred index. That editorial origin gives it strong lexical correctness for what it contains (synsets, hypernyms/hyponyms, antonyms, meronyms, etc.).  
- It has been used and stress‑tested in thousands of research and application projects for >25 years, which has exposed and fixed many obvious problems.

Main limitations (things that affect “accuracy” in real use)
1. Sense granularity (the biggest practical issue)  
   - WordNet senses are often very fine‑grained. Humans (and WSD systems) frequently disagree on which fine sense to pick for a word in context, so measured “accuracy” for word‑sense annotation can be low if you demand exact WordNet senses. Many researchers therefore collapse senses (coarse‑graining) or map to
",0,0,0,,0,,,,,,
IR_5,What is the trust value of the Linked Open Camera dataset?,0.5,"The trust value is 0.5 (on a 0–1 scale).
",1,1,1,,1,,,,,,
IR_5,How trustworthy is the Linked Open Camera dataset?,0.5,"Short answer: moderately trustworthy — it’s an official government corpus with high intrinsic data quality (accuracy, conciseness) and an operational SPARQL endpoint over HTTPS, but provenance and interlinking metadata are weak, so you should verify provenance before using it for critical tasks.

Key facts from the assessments
- Provider: Italian Chamber of Deputies (dati.camera.it). trusted source in principle.  
- Access: public SPARQL endpoint available (http://dati.camera.it/sparql) and many downloadable RDF dumps are advertised. HTTPS is supported and no authentication is required.
",0,0,0,,0,,,,,,
TA_1,Is the number of triples of NoiPA retrieved via the SPARQL endpoint always the same?,"No, they slightly increase over time.","Short answer: No — the number of triples retrieved from the NoiPA SPARQL endpoint has changed over time.

Evidence from the reports you provided (query-based triple counts at different dates):
- 2025-05-04: 412,894,535
- 2025-06-01: 419,389,676
- 2025-07-13: 432,861,574
- 2025-08-03: 439,796,613
- 2025-09-07: 446,170,057

Why the count can vary
- Data updates: the dataset is updated (reports show an update frequency or modification dates), so the graph grows/shrinks as data are added, removed or corrected.
- Different counting scopes: counts may include different named graphs or only the default graph. Some endpoint configurations treat the default graph as the union of named graphs, others do not.
- Inference/reasoning: counts can differ if reasoning/inference is enabled or disabled at measurement time.
- Temporary/administrative changes: maintenance, loading/unloading dumps, or transient replication states can change what the endpoint reports at a given moment.
- Measurement differences/timeouts: different queries/sampling or partial results due to timeouts may give different counts.

How to get a reproducible count
- Use a consistent SPARQL query and counting scope. For a union-over-all-graphs count (explicitly counting triples in all named graphs), run:
  SELECT (SUM(?c) AS ?total) WHERE {
    SELECT (COUNT(*) AS ?c) WHERE { GRAPH ?g { ?s ?p ?o } } GROUP BY ?g
  }
  Or, to count the default (current) graph:
  SELECT (COUNT(*) AS ?triples) WHERE { ?s ?p ?o }
- Run the same query multiple times and record timestamp to detect changes.
- For a stable snapshot, download the RDF dump (if available) and count triples locally (e.g., count N-Triples lines or use rdf tools). A dump gives a fixed snapshot independent of endpoint state.
- If you need historic stability, ask the dataset publishers for versioned dumps or snapshot identifiers.

If you want, I can:
- Provide exact SPARQL queries tailored to counting named graphs in Virtuoso (the endpoint NoiPA appears to run), or
- Suggest shell/command-line commands to count triples in an N-Triples/Turtle dump once you have it.
",1,1,1,,1,,,,,,
TA_1,The number of triples of NoiPA retrieved via the SPARQL endpoint is constant?,"No, they slightly increase over time.","Short answer: No — the triple count exposed by the NoiPA SPARQL endpoint is not constant. It changed over time in the reports you provided.

Evidence (reported query-based counts)
- 2025-05-04: 412,894,535
- 2025-06-01: 419,389,676
- 2025-07-13: 432,861,574
- 2025-08-03: 439,796,613
- 2025-09-07: 446,170,057

Why it varies
- The dataset is updated (new data added / removed) so total triples grow or shrink.
- Endpoint counting methods and which graphs are included can vary (default graph vs. all named graphs).
- Indexing/rebuild or maintenance on the triplestore can temporarily change reported counts.
- Some counts in metadata/VoID may reflect a snapshot (dump) while SPARQL counts reflect the live store.

How you can check the current count yourself
- Simple total-triples query:
  SELECT (COUNT(*) AS ?triples) WHERE { ?s ?p ?o }
- Per-graph counts (useful if the store uses named graphs):
  SELECT ?g (COUNT(*) AS ?triples) WHERE { GRAPH ?g { ?s ?p ?o } } GROUP BY ?g ORDER BY DESC(?triples)
- If the endpoint publishes VoID/DCAT metadata (or a service description), check that for a declared triple count and last-modified timestamp.

Practical suggestions
- If you need a stable value for analysis, take and record periodic snapshots (run the COUNT at scheduled intervals).
- To understand causes of jumps, compare per-graph counts and check modification dates in metadata or the publisher’s change log.
- If you want, I can produce the exact SPARQL queries formatted for the NoiPA endpoint and explain how to run them (including handling endpoint timeouts/limits). Would you like that?
",1,1,1,,1,,,,,,
TA_2,Is the SPARQL endpoint of DBLP always available?,Yes,"Short answer: No — there’s no guarantee it’s ""always"" available. However, the dblp SPARQL endpoint (https://sparql.dblp.org/sparql) has been observed online and responsive in multiple checks between May and September 2025, and it typically shows good latency and throughput.

Evidence from the quality checks (examples)
- Reports on 2025-05-04, 2025-06-01, 2025-07-13, 2025-08-03 and 2025-09-07 all show the SPARQL endpoint as Available and reasonably responsive (median latencies ~130–560 ms and several req/s throughput).
- The endpoint URL recorded in metadata: https://sparql.dblp.org/sparql
- Downloadable dumps are published (e.g. https://doi.org/10.4230/dblp.rdf.ntriples, https://dblp.org/rdf/dblp.ttl.gz) so you have an offline fallback.

Why it still isn’t “always available”
- Public endpoints have no formal SLA: they may go down for maintenance, upgrades, network issues, or transient overload.
- Automated checks sometimes disagree on HTTPS detection or dump availability (tooling/metrics can mis-detect even when the service is up).

Practical recommendations
- Don’t rely on 100% uptime for production-critical workflows. Use one or more of these mitigations:
  - Health-check frequently (simple ASK or lightweight SELECT) and implement retry/backoff.
  - Use the RDF dump(s) as a local fallback or for offline processing.
  - Cache results where feasible and respect endpoint usage policies (rate limits).
  - Monitor the endpoint with an external uptime service (e.g., Pingdom, UptimeRobot) or your own script.

Quick health-check examples
- HTTP header check:
  curl -I https://sparql.dblp.org/sparql
- Simple SPARQL ASK (returns true if endpoint answers):
  curl --data-urlencode 'query=ASK WHERE { ?s ?p ?o }' -H 'Accept: application/sparql-results+json' https://sparql.dblp.org/sparql

If you want, I can:
- Set up a small health-check script for you (cron-ready), or
- Help draft a robust client pattern (retries, caching, fallback to dump) for your application.
",0,0.5,0.5,,"0.5",,,,,,
TA_2,Is the DBLP SPARQL endpoint always up?,Yes,"I can’t check the endpoint live from here, so I can’t tell you its current moment‑to‑moment status. However:

- Historical scans in the reports you provided (2025‑05 through 2025‑09) show the DBLP SPARQL endpoint (https://sparql.dblp.org/sparql) as Available and responsive, with good median latency (roughly 130–560 ms) and throughput (~6–8 req/s). So it has been operational and reliable in those snapshots, but that doesn’t guarantee it’s “always up.”

If you want to check availability yourself (or set up monitoring), here are quick, practical checks and recommendations:

1) Quick manual checks
- HTTP health check:
  - curl -I https://sparql.dblp.org/sparql
    - Expect an HTTP 200 (or at least 405/OPTIONS supported). Non‑2xx often indicates a problem.
- Simple SPARQL query (ASK is cheap):
  - curl -s -X POST -H ""Accept: application/sparql-results+json"" --data-urlencode 'query=ASK WHERE { ?s ?p ?o }' https://sparql.dblp.org/sparql
  - A valid JSON true/false result indicates the endpoint is responding to queries.
- Or SELECT one triple:
  - curl -s -G --data-urlencode 'query=SELECT * WHERE { ?s ?p ?o } LIMIT 1' -H ""Accept: application/sparql-results+json"" https://sparql.dblp.org/sparql

2) Automate monitoring
- Use an external uptime monitor (UptimeRobot, Pingdom) to call the endpoint URL periodically.
- For query‑level checks, use Prometheus + Blackbox Exporter (HTTP probe) or a small script that issues ASK/SELECT queries every N minutes and alerts on failures/latency spikes.
- Record response codes, response times, and error messages (timeouts, 5xx, connection refused).

3) Production best practices
- Implement retries with exponential backoff and a reasonable timeout (e.g., 5–10s) in client code.
- Cache results where possible to reduce load and tolerate short outages.
- Respect any rate limits and prefer bulk dumps for heavy processing (DBLP provides RDF dumps).

If you want, I can:
- Draft a small monitoring script (bash or Python) you can run.
- Give an example Prometheus/Blackbox config.
- Attempt a live check now if you can run the curl commands and paste the output here.",0,0.5,0.5,,"0.5",,,,,,
TA_3,Is the data dump of the Allie Abbreviation And Long Form dataset always set to 1 in the observed period?,"No, it is discontinuous","No — not always.

Observed values for ""Availability of RDF dump (metadata)"" in the reports:
- 2025-05-04: 1 (dump indicated in metadata)  
- 2025-06-01: 1  
- 2025-07-13: 1  
- 2025-08-03: 1  
- 2025-09-07: 0 (dump reported offline / not available in metadata)

Notes:
- Across multiple reports the SPARQL-based check often could not confirm the dump (Availability of RDF dump (query) was False even when metadata said 1).
- On 2025-09-07 the metadata itself indicated the dump was unavailable, so the flag changed to 0.

If you want, I can extract the exact report snippets or check for other dates.
",1,1,1,,1,,,,,,
TA_3,Is the data dump of the Allie Abbreviation And Long Form dataset always up?,"No, it is discontinuous","Short answer: No — the dump is not reliably always up.

Why: across multiple quality scans (2025-05 → 2025-09) the dataset metadata consistently claims an RDF dump is available, but automated checks often cannot reach or verify it. Examples from the reports:
- 2025-06-01 / 2025-07-13 / 2025-08-03: metadata lists an RDF dump as online, but ""Availability of RDF dump (query)"" = False and no working download links were discovered via the SPARQL endpoint.
- 2025-09-07: the SPARQL endpoint was reported missing/offline and no dump was accessible at that time.

What to do to verify now
- Check the landing page: http://allie.dbcls.jp/
- Check the SPARQL endpoint: http://data.allie.dbcls.jp/sparql
- Try fetching the advertised dump URL (if you find it) and inspect headers:
  - curl -I <dump_url>  (look for HTTP 200, Content-Type, Last-Modified)
  - curl -fL -o /tmp/allie.dump <dump_url> (to download; -f exits on HTTP errors)
- Look for VoID/DCAT metadata or a download link on the landing page that gives an explicit dump URL and last-modified timestamp.

If you want ongoing assurance
- Monitor the dump URL with a simple uptime check (curl HTTP status + content-type) or use an external uptime-monitoring service set to check every X minutes and alert on failures.
- Optionally mirror the dump on your side (if license permits) and refresh on a schedule.

Contact for help
- Dataset contact in the reports: info AT dbcls.rois.ac.jp — you can ask them to confirm the canonical dump URL and whether dumps are continuously hosted.

If you want, I can draft a short email to the maintainers asking them to confirm dump URL, availability guarantee, and last-modified/update cadence.
",1,1,1,,1,,,,,,
TA_4,Is the accuracy of NoiPA constant over time?,No,"Short answer: Yes — as measured in the reports you provided, NoiPA’s accuracy is essentially constant and very high over time (≈0.9997 → 0.99966). Any differences are tiny and within measurement/sampling noise.

Details:
- 2025-05-04 — Accuracy: 0.9997/1
- 2025-06-01 — Accuracy: 0.9997/1
- 2025-07-13 — Accuracy: 0.99966/1
- 2025-08-03 — Accuracy: 0.99966/1
- 2025-09-07 — Accuracy: 0.99966/1

Interpretation and caveats:
- The reported “accuracy” here is an intrinsic, syntactic/structural quality score (checks such as empty annotations, whitespace in literals, malformed datatypes, functional/inverse‑functional property misuse). Those structural checks remain almost perfect across the dates.
- This does not guarantee semantic or factual correctness of the content (i.e., whether values are true in the real world). If you need to monitor factual accuracy or detect drifting errors, you’ll need domain-specific validation or reference-based checks over time.
- If you want, I can (a) produce a small plot of the time series, (b) list the individual subchecks behind the accuracy score, or (c) suggest additional checks to detect semantic/factual changes over time. Which would you prefer?
",1,1,1,,1,,,,,,
TA_4,Does the accuracy of NoiPA remain consistent over time?,No,"Short answer: Yes — the measured intrinsic accuracy of NoiPA stays essentially unchanged and very high across the reports you supplied (May → Sept 2025).

Details and evidence
- Reported Accuracy scores (intrinsic accuracy) by date:
  - 2025-05-04: 0.9997
  - 2025-06-01: 0.9997
  - 2025-07-13: 0.99966
  - 2025-08-03: 0.99966
  - 2025-09-07: 0.99966

- Sub-checks that drive the high accuracy score are consistently good across reports (examples):
  - ""triples with empty annotation"" — score ≈ 1.0
  - ""malformed data type literals"" — score ≈ 1.0
  - ""functional properties inconsistent"" — score ≈ 1.0
  - small variations exist in checks for whitespace in annotations (~0.998–0.999), but these are tiny.

Interpretation and caveats
- The accuracy metric is stable and near‑perfect in every snapshot — small numeric differences are within expected measurement/sampling noise, not signs of degradation.
- Accuracy here is an intrinsic/data-validity metric (syntactic/constraint checks, malformed literals, functional property violations). It does not cover other quality aspects that did vary or are weak for NoiPA (e.g., interlinking = 0, dereferenceability = 0, representational versatility, consistency warnings such as ontology‑hijacking).
- The presence of ontology-hijacking and undefined-term warnings affects the dataset’s Consistency score, not the specific Accuracy values reported; those issues are worth investigating because they may eventually affect downstream correctness or interoperability even if immediate accuracy checks pass.

If you want
- I can plot the accuracy trend (simple line chart) or extract and compare the underlying sub-metrics over time.
- I can also produce a short checklist of concrete actions to keep or improve accuracy (and address the consistency/interoperability issues). Which would you prefer?
",1,1,1,,1,,,,,,
TA_5,What is the longest continuous period in which the Allie Abbreviation And Long Form dataset data dump has been equals to 1?,91 days,"According to the provided reports, the RDF-dump metadata flag was 1 on 2025-05-04, 2025-06-01, 2025-07-13 and 2025-08-03, and then dropped to 0 by 2025-09-07.

Thus the longest continuous period with the dump flag = 1 was from 2025-05-04 through 2025-08-03 (inclusive) — 92 days.

If you meant the query-based dump availability flag (Availability of RDF dump (query)), that is False in all checked reports; let me know which metric you want if I should re-check.
",1,1,1,,1,,,,,,
TA_5,"For how long, without interruption, has the Allie Abbreviation And Long Form dataset data dump remained up?",91 days,"Short answer: I can’t confirm any uninterrupted uptime. Metadata asserted a dump was available between the May–Aug 2025 checks (roughly 2025-05-04 → 2025-08-03, ≈3 months), but query-based checks never found a working download and by 2025-09-07 the metadata reported the RDF dump as missing/offline.

Evidence from the reports:
- 2025-05-04, 2025-06-01, 2025-07-13, 2025-08-03: dataset metadata indicated an RDF dump was available (Availability of RDF dump (metadata) = 1), but the SPARQL/query checks did not confirm an accessible dump and no working download URLs were discovered.
- 2025-09-07: metadata reported the RDF dump as offline/missing (Availability of RDF dump (metadata) = 0).

What to do to get a definitive uninterrupted uptime value
- Locate the actual dump download URL (or the server hosting it).  
- Check HTTP headers (Last-Modified, ETag) and server logs for continuous availability.  
- Use periodic monitoring (e.g., daily HEAD requests) or an uptime service to measure uninterrupted availability going forward.  
- If you need historic verification, ask the dataset publisher (Database Center for Life Science / Allie team) for access logs or release timestamps.

If you want, I can attempt to (re)probe the declared endpoints/dump URLs now (or draft an email to the publisher requesting logs/confirmation). Which would you prefer?
",1,1,1,,1,,,,,,
Co_1,Which dataset has the highest number of triples?,DBLP,"The dblp Knowledge Graph (kg id: dblp-kg) has the most triples — about 1.45 billion triples (≈1.44–1.45 × 10^9 triples reported via SPARQL).
",1,1,1,,1,,,,,,
Co_1,Which is the biggest dataset among the compared ones?,DBLP,"The dblp Knowledge Graph is the largest — about 1.4–1.45 billion triples (≈1.45×10^9 by SPARQL query). For comparison, NoiPA is the next largest (~0.44–0.45 billion triples), followed by Allie (~0.30–0.31 billion), BPR (~0.35 billion by query), Coronavirus (~0.08–0.081 billion), LiLa Lemma Bank (~1.7 million), and WordNet 2.0 (~0.71 million).
",1,1,1,,1,,,,,,
Co_2,Which is the dataset performing worst in the license metric?,CIDOC-CRM,"CIDOC‑CRM performs worst on the licensing metric — score 0.0/1. The reports show no machine‑readable license detected in its metadata.
",1,1,1,,1,,,,,,
Co_2,Which is the worst performing dataset in the license metric?,CIDOC-CRM,"CIDOC-CRM. Its licensing score is 0.0/1 — no machine‑readable license was detected (and no human‑readable license found in the metadata), so it ranks worst on the licensing metric.
",1,1,1,,1,,,,,,
Co_3,What is the mean score for the accuracy dimension?,0.50,"Using the most recent report for each KG in the document (NoiPA, dblp, LiLa Lemma Bank, WordNet 2.0 (W3C), Coronavirus, CIDOC‑CRM, Allie, BPR), the Accuracy scores are:

- NoiPA: 0.99966  
- dblp: 1.00  
- LiLa Lemma Bank: 0.00  
- WordNet 2.0 (W3C): 0.00  
- Coronavirus: 1.00  
- CIDOC‑CRM: 0.00  
- Allie: 0.00  
- BPR: 1.00

Mean accuracy = (0.99966 + 1 + 0 + 0 + 1 + 0 + 0 + 1) / 8 = 3.99966 / 8 ≈ 0.49996.

So the mean Accuracy score ≈ 0.50 (≈49.996%).
",1,1,1,It only used scores from the last check ,1,,,,,,
Co_3,What is the mean value of the accuracy dimension?,0.50,"The mean value of the accuracy dimension is 0.0.
",0,0,0,,0,,,,,,
Co_4,What is the best observed score in the Verifiability dimension?,"0.4983 (NoiPA, Allie, Bathing Water Quality)","The highest Verifiability score observed is 0.4983333333333333 (≈49.83%). It appears for NoiPA (and the Allie report shows the same rounded value: 0.4983).
",0.5,0.5,0.5,,"0.5",,,,,,
Co_4,What is the highest observed Verifiability score?,"0.4983 (NoiPA, Allie, Bathing Water Quality)","The highest observed Verifiability score is 0.498333
",0.5,0.5,0.5,,"0.5",,,,,,
Co_5,What are the highest-performing dimensions in September 2025?,"License, Performance, Accuracy, Believability, Currency, Timeliness, Security, Conciseness and Interpretability","Short answer
- The strongest, consistently high‑scoring dimensions in September 2025 are intrinsic quality (Accuracy and Conciseness), and — for datasets that expose a working endpoint — Performance and basic Accessibility/Availability (SPARQL endpoint + downloadable dumps in metadata).  
- FAIR sub‑principles Findability/Accessibility/Reusability (F/A/R) are also generally strong across many datasets.

Evidence (selected examples from the 2025‑09‑07 reports)
- Accuracy (Intrinsic)
  - dblp: 1.0
  - Coronavirus: 1.0
  - NoiPA: 0.99966
  - Environment Agency: ~0.99999
  - BPR: 1.0
  → Accuracy measures are near‑perfect across most assessed KGs.

- Conciseness (Intrinsic)
  - dblp: ~0.994
  - Coronavirus: ~0.9942
  - NoiPA: ~0.99135
  - Environment Agency: ~0.9936
  → Both extensional and intensional conciseness are very high.

- Performance & Availability (when endpoints are present)
  - dblp: Performance = 1.0 (low latency, solid throughput)
  - NoiPA: Performance = 1.0
  - Coronavirus: Performance ~0.50 (moderate)
  - BPR: Performance ~0.5075
  → Datasets with active SPARQL endpoints typically show good performance and Availability scores.

- FAIR (aggregate)
  - dblp FAIR ≈ 3.46/4
  - Environment Agency FAIR ≈ 3.77/4
  - Coronavirus FAIR ≈ 3.0/4
  → The F, A and R sub‑scores are generally high for many datasets, reflecting good metadata, IDs, licensing and registration.

What remains weak (brief)
- Interlinking (very low across most datasets)
- Understandability (few datasets expose comprehensive vocab lists or label coverage)
- Dynamicity (modification dates / update frequencies often missing)
- Consistency flags (ontology‑hijacking and undefined term warnings in several KGs)
- HTTPS / security detection was inconsistent for some endpoints

Recommended focus
- Maintain the strong practices that produce high Accuracy/Conciseness (validation, canonicalization).
- Prioritize improvements in interlinking, vocab documentation and explicit dynamicity metadata to lift weaker dimensions that limit reuse and integration.
",0,0,0,,0,,,,,,
Co_5,What are the dimensions performing better in September 2025?,"License, Performance, Accuracy, Believability, Currency, Timeliness, Security, Conciseness and Interpretability","Short answer — across the datasets assessed on/around 2025‑09‑07 the strongest (best‑performing) dimensions were:

- Intrinsic: Accuracy and Conciseness  
- Accessibility (especially Availability / SPARQL endpoint responsiveness and Performance)  
- FAIR subprinciples: Findability, Accessibility and Reusability (F / A / R) — and machine‑readable licensing in particular  
- Trust/Believability (metadata about provider / clear descriptions) — moderate but consistent

Evidence / quick examples (Sep 07 snapshots)
- dblp: Accuracy = 1.0; Conciseness ≈ 0.994; Performance = 1.0; strong FAIR (FAIR ≈ 3.46).  
- Coronavirus (micro‑coronavirus): Accuracy = 1.0; Conciseness ≈ 0.994; high URI dereferenceability (~0.96); Licensing = 1.0; good FAIR (≈3.0).  
- NoiPA: very high Accuracy (~0.9997) and Conciseness (~0.991); Performance = 1.0; good publisher / license metadata.  
- LiLa Lemma Bank: strong FAIR F/A/R signals and machine‑readable license present (but many other metrics unassessed because the SPARQL endpoint was treated as offline).  
- CIDOC‑CRM, BBC Programmes, Environment Agency BWQ: these had many zero/unassessed dimensions (no working endpoint or missing dumps
",0,0,0,,0,,,,,,
E&D_1,Why is the availability score equals to 1 for the DBLP dataset?,Because all the metrics in the availability dimension can be correctly computed,"Short answer: because the automated availability checks for dblp passed the main criteria used to compute that dimension: the SPARQL endpoint is reachable, the metadata declares an RDF dump and download availability, and the dataset's HTTP URIs were found to be dereferenceable. Those positive signals (especially endpoint availability + declared dump/download in metadata + dereferenceability) drive the Availability score to 1.0 even though some query-level dump checks returned False.

Details / evidence from the 2025-08-03 report
- SPARQL endpoint: Available
- Availability of RDF dump (metadata): 1 (dump declared in metadata)
- Availability for download (metadata): 1 (download declared in metadata)
- URIs Deferenceability: 1.0
- Query-level checks: Availability of RDF dump (query) = False and Availability for download (query) = False — the endpoint did not confirm the dump via SPARQL, but the metadata-level signals were present and positive.

Why that leads to a full availability score
- The Availability dimension is computed from a small set of checks (endpoint presence, dump and download availability as reported in metadata or discovered, and dereferenceability). In this run the metadata- and endpoint-based checks were sufficient and weighted to give the maximum score.
- Query-level confirmation of the dump is considered separately and can be False without reducing Availability to zero if other availability indicators are strong.

Caveats and suggestions
- You may see different Availability values in other runs/dates because the checker uses a mix of metadata- and query-based signals and network/endpoint behavior can change. The reports you provided show that on other dates the same dataset sometimes had Availability = 0.75 or 0.5 when some metadata or dereferenceability checks failed.
- To make availability more robust and avoid ambiguity, ensure:
  - The RDF dump is actually reachable via the advertised download URL(s) (and that the SPARQL endpoint can reference it if the checker expects that).
  - The dataset metadata (VoID/DCAT) lists concrete download URLs and media types.
  - The endpoint reliably responds and URIs remain dereferenceable.

If you want, I can:
- Extract the VoID/DCAT metadata fields the checker used (dump/download URLs and media types) and show which ones were detected, or
- Suggest precise SPARQL/HTTP checks you can run to reproduce the availability test.
",1,1,1,,1,,,,,,
E&D_1,Why does the DBLP dataset have an availability score of 1?,Because all the metrics in the availability dimension can be correctly computed,"Short answer: Because in the run that produced an Availability = 1.0 the DBLP KG met the main availability checks the scorer weights most heavily — an operational SPARQL endpoint was reachable, the metadata advertises an RDF dump and download links, and URIs were dereferenceable. Those positive results outweighed the single failing query-based dump check, so the linear combination of the availability sub‑metrics reached the maximum.

Details and evidence from the report
- The 2025-08-03 report shows:
  - Sparql endpoint: Available
  - Availability of RDF dump (metadata): 1
  - Availability for download (metadata): 1
  - URIs Dereferenceability: 1.0
  - Availability of RDF dump (query): False (the endpoint did not confirm the dump via a query)
- The Availability dimension is computed from multiple submetrics (endpoint presence, dump advertised in metadata, dump discoverable via endpoint/query, download links in metadata, URI dereferenceability). In that run the majority of the submetrics relevant to availability were true (and those appear to be weighted sufficiently), producing an overall score of 1.0.

Why the dump query check can be False while Availability = 1
- The checker distinguishes metadata-level evidence (is a dump advertised / are download links present) from query-level evidence (can the dump be retrieved or confirmed via SPARQL). If metadata clearly lists a downloadable dump and the endpoint exists and URIs are dereferenceable, the overall availability score can still be maximal even if the query-based verification of the dump failed. In other words, metadata + endpoint + dereferenceability are enough to reach the top score in that configuration.

Why Availability sometimes differs across dates
- You have later reports (e.g., 2025-09-07 / 2025-06-01) showing Availability = 0.75 or 0.5. Those lower scores reflect a different mix of submetric results in those runs (for example the dump may not have been advertised in metadata in that run, or other submetrics changed). The availability score is a linear combination of the submetric values, so changes in any of them (especially the ones the system weights more) will change the overall availability.

If you want the availability assessment to be unambiguously high
- Ensure the RDF dump is both advertised in the dataset metadata (VoID/DCAT) and directly accessible (confirmable via the SPARQL endpoint or via the listed download URL).
- Keep the SPARQL endpoint reliably online.
- Maintain dereferenceable URIs.
- Include explicit download URLs and a VoID/DCAT distribution description so both metadata- and query-based checks can confirm the dump.

If you’d like, I can:
- Point to the exact lines in the report you supplied that justify Availability = 1, or
- Suggest the minimal VoID/DCAT triples and SPARQL checks that will make the dump verifiable by both metadata- and query-based tests.
",1,1,1,,1,,,,,,
E&D_2,Does a score of amount of data equals to 0 mean that the dataset is empty?,"No, it means that the amount of data cannot be computed via the SPARQL endpoint and it is not declared in the metadata.","Not necessarily.

A 0 for the ""amount of data"" score usually means the scoring procedure could not validate any of the quantity metrics it uses (or the measured values map to the lowest point on the chosen normalization), not that the KG is necessarily empty. Common reasons:

- Missing or unreadable inputs: the score builder needs values such as number of triples (from VoID/metadata or a SPARQL COUNT query), number of entities, number of properties, etc. If those inputs are absent or the SPARQL endpoint is offline, the tool may assign 0.  
- Normalization/thresholds: the raw counts are mapped to [0,1]. Very small counts or counts below the scoring threshold can produce a near-zero or zero normalized score.  
- Measurement error: timeouts, permission errors, or parsing failures can force the score to 0 even though data exist.

How to check if the dataset is actually empty
1. Check the report fields first:
   - Look for ""Number of triples (query)"" and ""Number of triples (metadata)"". If either shows a positive integer, the dataset is not empty even if the amount-of-data score is 0.
2. If a SPARQL endpoint is available, run counts:
   - Count triples:
     SELECT (COUNT(*) AS ?triples) WHERE { ?s ?p ?o }
   - Count distinct subjects/entities:
     SELECT (COUNT(DISTINCT ?s) AS ?subjects) WHERE { ?s ?p ?o }
   - Count distinct properties:
     SELECT (COUNT(DISTINCT ?p) AS ?properties) WHERE { ?s ?p ?o }
3. If there’s a dump (NT/Turtle), you can:
   - For N‑Triples: wc -l file.nt (rough estimate of triples)
   - For compressed dumps: zcat file.nt.gz | wc -l
4. Check VoID/DCAT metadata for void:triples / dcterms:modified / distribution links.

If you want, tell me the dataset (or the SPARQL endpoint URL) you want me to check and I can suggest the exact query/commands to run.
",0.5,0,0.5,,"0.5",,,,,,
E&D_2,"If the amount-of-data score is 0, does that mean the dataset contains no data?","No, it means that the amount of data cannot be computed via the SPARQL endpoint and it is not declared in the metadata.","Not necessarily. A 0 for the “amount of data” score usually means the assessment could not verify or credit any of the quantity metrics used to compute that score — not that the dataset is definitely empty. The score is derived from inputs such as number of triples (from metadata and/or a SPARQL count), number of entities, number of properties, etc. If those inputs are missing, unreachable, or below the scoring thresholds, the aggregated amount‑of‑data score can become 0.

Common reasons you might see a 0 amount‑of‑data score
- No usable triple count was found in metadata (no void:triples / dcat:triples) and the SPARQL endpoint was absent or failed, so the checker couldn’t run a COUNT query.  
- The SPARQL endpoint exists but the tool failed to get a reliable count (timeouts, permission issues, very large query cost).  
- The dataset is extremely small compared to the scoring scale or falls below the minimum threshold used for positive scoring.  
- The metric implementation treats missing values as zero when combining metrics.

How to tell which of those applies
- Check the raw metrics reported alongside the score: do you see a “Number of triples (metadata)” or “Number of triples (query)” value, or are they “-” or False? If counts are present and >0, the dataset has data even if the aggregated score was 0.  
- If you have SPARQL access, run a simple count: 
  SELECT (COUNT(*) AS ?triples) WHERE { ?s ?p ?o }  
  If that returns >0, the graph contains triples.  
- Look for VoID/DCAT metadata (void:triples, dcat:distribution) or an RDF dump and inspect it (wc, triple counter).

What to do to get a non‑zero amount‑of‑data score
- Publish a triple count in VoID/DCAT (void:triples or dcat metadata).  
- Ensure a working SPARQL endpoint that allows a COUNT query, or provide a downloadable RDF dump and declare its media type.  
- Make sure the endpoint is reachable (fix timeouts, increase query limits) so automated checkers can retrieve counts.  
- If the dataset is small but valid, document that explicitly (so scoring can reflect the presence rather than treat it as missing).

In short: a 0 amount‑of‑data score often signals missing/failed measurements or very small size, not definitive emptiness. Verify the underlying reported metrics (triple counts, dumps, endpoint availability) to know whether the dataset actually contains data.
",0.5,0,0.5,,"0.5",,,,,,
E&D_3,"When the availability score is 1, does it mean that both the SPARQL endpoint and the data dump are working?",Yes,"Short answer: usually yes — an availability score of 1 means the dataset passed all the individual availability checks (SPARQL endpoint, RDF dump presence/downloadability, URI dereferenceability) used by the framework. In other words, the endpoint is reachable and the dump(s) are reported as available and downloadable.

Details and caveats:
- Availability is an aggregate (a linear combination) of several metrics: SPARQL endpoint status, RDF dump presence in metadata, RDF dump availability via the endpoint (query), availability for download (metadata and query), and URI dereferenceability. A score of 1 means those component metrics evaluated to their best values.
- Because it is an aggregate, you should inspect the per-metric breakdown to confirm what “available” means in your case. For example:
  - SPARQL endpoint: Available / Offline / - (missing)
  - RDF dump (metadata): 1 (online) / 0 (offline) / -1 (missing)
  - RDF dump (query): True / False / - (missing)
  - Availability for download (query/metadata): True / False / -
  - URIs dereferenceability: 1/0/-
- Implementation/weighting differences: the framework computes a linear combination of these metrics. In some implementations a strong result on some metrics could compensate others, so theoretically an availability score might be high even if one metric is imperfect. In practice, a score of 1 typically indicates all availability checks succeeded.
- Examples from your reports illustrate the difference: WordNet had availability ≈0.5 because the dump was indicated in metadata but the SPARQL endpoint was missing; dblp and some other datasets scored 1 because the endpoint responded and dumps/URIs were available.

Recommendation: when you see availability = 1, open the detailed availability section and verify each individual metric (endpoint: Available, dump: online/downloadable, URIs dereferenceable) to confirm the exact state.
",1,1,1,,1,,,,,,
E&D_3,"if the availability score is 1, does that mean both the SPARQL endpoint and the data dump are up?",Yes,"Not necessarily — you must check the underlying availability submetrics.

The Availability score is a composite (linear combination) of several checks such as:
- SPARQL endpoint (is it online/responding)
- RDF dump announced in metadata (VoID/DCAT)
- RDF dump verifiable/downloadable via query or listed download links
- Availability-for-download flags (metadata vs. query)
- URI dereferenceability

So an Availability = 1 typically means the implementation’s weighting of those checks reached the maximum (i.e., the endpoint is working and the dereferenceability/download signals that matter for that calculation are satisfactory). But in practice you can see cases where the metadata says a dump exists (metadata-based checks = true) while the dump could not be verified via the SPARQL-based check (query-based dump check = False). Depending on how the composite score is computed, that can still yield an overall Availability score of 1.

What to do to be sure:
- Inspect the individual availability metrics in the report: Sparql endpoint, Availability of RDF dump (metadata), Availability of RDF dump (query), Availability for download (metadata/query), and URIs Deferenceability.
- Manually test:
  - Open the SPARQL endpoint URL and run a simple query.
  - Fetch the dump URLs from the metadata (curl/wget/HEAD) and confirm HTTP 200 and a suitable RDF media-type (text/turtle, application/n-triples, application/ld+json, etc.).
  - Test dereferencing a sample resource URI (HTTP GET, content negotiation).

In short: Availability = 1 is a strong signal the dataset is accessible, but always confirm the specific submetrics to verify the dump is actually reachable (not just declared).
",0,0,0,,0,,,,,,
E&D_4,"if license is set to 1, does it mean that an open license is used?","No, it means that a license is specified","Not automatically. A license metric equal to 1 normally means the assessment found a license statement (e.g., a machine‑readable license URI or a human‑readable license) and marked that check as satisfied — it does not by itself prove the licence is ""open"" in the sense of the Open Definition.

Quick checklist to tell whether a found license is actually open
- Inspect the license identifier/URI. Common open ones include:
  - CC0, CC BY, CC BY‑SA (Creative Commons) — generally considered open.
  - Permissive licences like Apache 2.0, MIT — open.
- Reject / treat as non‑open if you see restrictions like:
  - NC (non‑commercial), ND (no derivatives) — these are not open under the Open Definition.
- Verify against authoritative lists:
  - Open Definition / Open Knowledge Foundation list of approved open licences.
  - SPDX registry (for canonical identifiers like ""CC-BY-4.0"", ""Apache-2.0"").
- Confirm machine vs human readable:
  - A metric of 1 for a machine‑readable license means the license URI was found in metadata. Still follow the URI to read the license text/conditions.
  - A human‑readable license (web page) is good evidence, but still check the terms.
- Check jurisdictional or custom variants: local or custom licenses may look like standard ones but impose extra clauses; read the text.

Examples
- If the metric found ""https://creativecommons.org/publicdomain/zero/1.0/"" (CC0) → open.
- If it found ""https://creativecommons.org/licenses/by‑nc/4.0/"" (CC BY‑NC) → not open (NC = non‑commercial).
- If it found no license or only a generic statement like ""All rights reserved"" → not open.

If you want, I can:
- Check a specific dataset/license from your list and tell you whether it is open, or
- Give a short script/regexp or SPDX list to programmatically classify license URIs as open vs non‑open.
",1,1,1,,1,,,,,,
E&D_4,"If license is set to 1, does it mean that an open license is used?","No, it means that a license is specified","Short answer: No — a licensing score of 1 usually means “a license was detected (and retrievable)” according to the metric’s criteria, not necessarily that the license is an open license.  

Explanation and how to check
- What the metric typically measures: the licensing metric in your reports (R1.1 / ""License machine- or human-readable retrievable"") rewards that a licence statement exists and can be retrieved from a primary source (VoID/DCAT metadata, dct:license / schema:license, or the SPARQL endpoint). A value of 1 means the detector found a license and could read it.  
- What it does not automatically guarantee: that the license is ""open"" in the sense of the Open Definition (i.e., allows free reuse, redistribution, and derivative/ commercial use as appropriate). The metric rarely performs a legal-openness classification beyond presence and machine-readability.

How to tell whether a detected license is open
1. Inspect the license identifier/URI found in metadata (common predicates: dct:license, dc:license, schema:license, void:license).  
2. Compare that URI or name to a list of known open licenses (examples below). If it matches, it is considered open. If it’s a custom license or an uncommon URL, read the license text and check whether it meets the Open Definition (permits reuse, redistribution, and derivatives; not unduly restrictive).  
3. Quick rule-of-thumb: Creative Commons “BY” and “BY-SA” and public-domain (CC0 / PDDL) are open; “NC” (non-commercial) and “ND” (no-derivatives) are not considered open. Database-specific open licences (e.g., ODbL) are normally considered open (but carry share‑alike requirements).

Common license URIs typically considered open
- CC0 / Public domain: https://creativecommons.org/publicdomain/zero/1.0/ (or opendefinition.org/licenses/cc-zero)  
- Creative Commons Attribution: https://creativecommons.org/licenses/by/4.0/ (open; requires attribution)  
- Creative Commons Attribution-ShareAlike: https://creativecommons.org/licenses/by-sa/4.0/ (open; share‑alike)  
- Open Data Commons Attribution License (ODC-BY) and Open Database License (ODbL): https://opendatacommons.org/licenses/odc-by/ and https://opendatacommons.org/licenses/odbl/ (open; ODbL imposes share‑alike on DBs)  
- PDDL (ODC Public Domain Dedication & Licence): https://opendatacommons.org/licenses/pddl/ (public domain-like)

Examples of license strings/URIs that are NOT open
- Any Creative Commons license with NC (non-commercial) or ND (no derivatives), e.g. https://creativecommons.org/licenses/by-nc/4.0/ or https://creativecommons.org/licenses/by-nd/4.0/  
- Proprietary or restrictive publisher licenses, or custom licences whose terms restrict reuse or redistribution.

Practical checklist you can use
- Is there a license URI in metadata? (If yes, metric = 1.)  
- Does that URI match a known open license from the list above? (If yes → open.)  
- If it’s a custom license: read whether it allows redistribution, reuse and derivative/commercial use; if it forbids those, it is not open.  
- If you want, paste the license URI or text here and I’ll classify it (open / not open) and explain why.

If you want, tell me a specific dataset or license URI from the reports and I’ll check whether it is an open license.
",1,1,1,,1,,,,,,
E&D_5,"If verifiability is set to 1, does it mean that all information about providers are set?",Yes,"Short answer: not necessarily. A verifiability score = 1 in this framework means the verifiability sub‑metrics used by the assessor are present and scored maximally according to the tool’s rules — it does not by itself prove every possible provider field exists or that the provider information is correct.

Details you should know
- What the verifiability metric typically covers (the submetrics used in your reports):
  - Author (query): author(s) discovered by querying the SPARQL endpoint.
  - Author (metadata): author(s) declared in VoID/DCAT/metadata documents.
  - Contributor: contributors declared in endpoint/metadata.
  - Publisher: publisher declared in endpoint/metadata.
  - Sources: declared source(s) / contact info (URL, name, email) in metadata.
  - Signed: whether the dataset is cryptographically signed (or a similar “signed” flag) in metadata.
- How a score of 1 is computed: the framework combines the individual submetric scores (often as a weighted linear combination). Score = 1 means each submetric reached its maximum according to the tool (e.g., author present, publisher present, sources present, signed = true).
- What it does not guarantee:
  - It does not prove the provider information is accurate, authoritative, or up‑to‑date — only that the expected metadata fields were present and parsable.
  - It does not guarantee every conceivable provider field is present (e.g., ORCID, postal address, multiple contact persons) — only those specific submetrics the framework checks.
  - It may be affected by where the tool looks (metadata vs. endpoint). If the SPARQL endpoint is missing, some submetrics might be scored from metadata only, or the score may be unattainable.

Practical checks you can run to be sure
- Inspect VoID/DCAT metadata and the SPARQL endpoint’s metadata for explicit predicates: dct:creator / dct:publisher, foaf:maker, prov:wasAttributedTo, foaf:homepage, foaf:mbox (or contact emails).
- Verify the provider URL(s) actually resolve and represent the same organization.
- Cross‑check author/publisher emails and names vs. the dataset landing page or institutional site.
- Check for a machine‑readable signature or provenance statement (e.g., W3C Verifiable Credentials, PGP signature, dcterms:rights + cryptographic signature) if “Signed” is required.
- If you need trust (not just verifiability), validate the contact and provider through independent sources (institutional pages, DOI registration, known trusted provider lists, ORCID records).

If you want, I can:
- Show the exact metadata properties this particular assessment uses for each submetric, or
- Inspect a dataset’s VoID/DCAT or SPARQL metadata (you can paste a sample) and tell you which verifiability submetrics are present or missing.
",1,1,0.5,,1,,,,,,
E&D_5,"If the verifiability score is 1, does it mean that all provider information is available?",Yes,"Short answer: no — a verifiability score of 1 means the verifiability metrics included in the computation all achieved their maximum values according to the automated checks, not that the dataset provider information is necessarily complete, correct, or independently authenticated.

Details and caveats:
- What a score of 1 does mean
  - The verifier found the expected provider metadata (e.g., author/creator, contributor, publisher, source, license and/or signature fields as defined by the tool) in the inspected sources (VoID/metadata and/or SPARQL endpoint) and each individual verifiability metric received the best possible value.
  - In other words: the automated checks succeeded for every verifiability sub-metric that was part of the scoring function.

- What a score of 1 does NOT guarantee
  - It does not guarantee the information is accurate, up-to-date, or belongs to the claimed party. The check is syntactic/presence-based unless the tool explicitly validates external facts (it usually does not).
  - It does not ensure authenticity (e.g., a dataset can claim a publisher or provide an email address without digital signing). The “signed” metric is separate and must itself be true to indicate signatures.
  - It does not mean every conceivable provider field is supplied — only that the metrics used in the linear combination are satisfied. Different implementations may include different sub-metrics.
  - If some inputs were unavailable (e.g., SPARQL endpoint missing) some metrics may be treated as “not applicable” and the score can still reach 1 based on the remaining metrics — check the per-metric outputs to be sure.

- How to confirm real provider coverage (recommended checks)
  - Inspect the per-metric outputs (Author (query), Author (metadata), Contributor, Publisher, Sources, Signed) — these show exactly which fields were found and where.
  - Verify the provider URLs and contact emails manually (visit the URL, send a test/clarification email if appropriate).
  - Check for machine-readable provenance predicates (dcterms:creator/dcterms:publisher, prov:wasAttributedTo, foaf:maker) in both VoID/DCAT and dataset content.
  - Confirm digital signatures or verifiable assertions if you need authenticity (e.g., RDF Signatures, Linked Data Signatures, signed distributions).
  - Cross-check publisher identity against trusted provider lists or institutional pages (to reduce spoofing risk).

- If you control the dataset and want a true/verifiable 1
  - Ensure provider metadata is present both in the dataset metadata (VoID/DCAT) and exposed via the SPARQL endpoint.
  - Include stable identifiers for authors/publishers (ORCIDs, ROR IDs, institutional URLs).
  - Publish machine-readable provenance and contact information, and sign releases where feasible.

If you want, I can:
- Look at the per-metric verifiability outputs for a particular dataset from your report and explain which sub-metrics passed or failed; or
- Suggest RDF/VoID/DCAT snippets to add so automated verifiability checks will recognize the provider information.
",1,0,0,allucinates in the initial part of the response,0,,,,,,